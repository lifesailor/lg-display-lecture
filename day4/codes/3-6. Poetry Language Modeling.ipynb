{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 500\n",
    "LATENT_DIM = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "for line in open('./robert_frost.txt', encoding='utf-8'):\n",
    "    line = line.rstrip()\n",
    "    if not line:\n",
    "        continue\n",
    "        \n",
    "    input_line = '<sos> ' + line\n",
    "    target_line= line + ' <eos>'\n",
    "    \n",
    "    input_texts.append(input_line)\n",
    "    target_texts.append(target_line)\n",
    "\n",
    "all_lines = input_texts + target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1436 1436 2872\n"
     ]
    }
   ],
   "source": [
    "print(len(input_texts), len(target_texts), len(all_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentences\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "tokenizer.fit_on_texts(all_lines)\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length:  12\n"
     ]
    }
   ],
   "source": [
    "# find max seq length\n",
    "max_sequnce_length_from_data = max(len(s) for s in input_sequences)\n",
    "print(\"Max sequence length: \", max_sequnce_length_from_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3056 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# get_word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(word2idx))\n",
    "assert('<sos>' in word2idx)\n",
    "assert('<eos>' in word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor:  (1436, 12)\n"
     ]
    }
   ],
   "source": [
    "# pad sequences so that wee get a N x T matrix\n",
    "max_sequence_length = min(max_sequnce_length_from_data, MAX_SEQUENCE_LENGTH)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
    "print(\"Shape of data tensor: \", input_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load in pre-trained word vectors\n",
    "print(\"Loading word vectors...\")\n",
    "word2vec = {}\n",
    "\n",
    "with open(os.path.join('./glove.6B.%sd.txt' % EMBEDDING_DIM), encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        \n",
    "print(\"Found %s word vectors.\" % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filling pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print(\"filling pre-trained embeddings\")\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1436 12 3000\n"
     ]
    }
   ],
   "source": [
    "print(len(input_sequences), max_sequence_length, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_targets\n",
    "one_hot_targets = np.zeros((len(input_sequences), max_sequence_length, num_words))\n",
    "for i, target_sequence in enumerate(target_sequences):\n",
    "    for t, word in enumerate(target_sequence):\n",
    "        if word > 0:\n",
    "            one_hot_targets[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained embeddings into Embedding layer\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model\n",
    "input_ = Input(shape=(max_sequence_length,)) # encoder\n",
    "initial_h = Input(shape=(LATENT_DIM,)) # hidden_state\n",
    "initial_c = Input(shape=(LATENT_DIM,)) # cell_state\n",
    "x = embedding_layer(input_)\n",
    "lstm = LSTM(LATENT_DIM, return_state=True, return_sequences=True)\n",
    "x, _, _ = lstm(x, initial_state=[initial_h, initial_c])\n",
    "dense = Dense(num_words, activation='softmax')\n",
    "output = dense(x)\n",
    "\n",
    "# 학습만 하고 출력은 안할 것이다.\n",
    "model = Model([input_, initial_h, initial_c], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 12, 100)      300000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 12, 25), (No 12600       embedding_1[0][0]                \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 12, 3000)     78000       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 390,600\n",
      "Trainable params: 390,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1148 samples, validate on 288 samples\n",
      "Epoch 1/500\n",
      "1148/1148 [==============================] - 2s 2ms/step - loss: 5.4038 - acc: 0.0543 - val_loss: 5.0868 - val_acc: 0.0095\n",
      "Epoch 2/500\n",
      "1148/1148 [==============================] - 1s 666us/step - loss: 4.6417 - acc: 0.0306 - val_loss: 4.7954 - val_acc: 0.0833\n",
      "Epoch 3/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 4.3444 - acc: 0.0833 - val_loss: 4.9071 - val_acc: 0.0833\n",
      "Epoch 4/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 4.3007 - acc: 0.0833 - val_loss: 4.9262 - val_acc: 0.0833\n",
      "Epoch 5/500\n",
      "1148/1148 [==============================] - 1s 704us/step - loss: 4.2493 - acc: 0.0833 - val_loss: 4.9741 - val_acc: 0.0833\n",
      "Epoch 6/500\n",
      "1148/1148 [==============================] - 1s 712us/step - loss: 4.2192 - acc: 0.0833 - val_loss: 4.9878 - val_acc: 0.0833\n",
      "Epoch 7/500\n",
      "1148/1148 [==============================] - 1s 704us/step - loss: 4.1889 - acc: 0.0833 - val_loss: 5.0007 - val_acc: 0.0833\n",
      "Epoch 8/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 4.1612 - acc: 0.0833 - val_loss: 5.0133 - val_acc: 0.0833\n",
      "Epoch 9/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 4.1351 - acc: 0.0833 - val_loss: 5.0265 - val_acc: 0.0830\n",
      "Epoch 10/500\n",
      "1148/1148 [==============================] - 1s 700us/step - loss: 4.1086 - acc: 0.0833 - val_loss: 5.0336 - val_acc: 0.0830\n",
      "Epoch 11/500\n",
      "1148/1148 [==============================] - 1s 695us/step - loss: 4.0799 - acc: 0.0833 - val_loss: 5.0367 - val_acc: 0.0828\n",
      "Epoch 12/500\n",
      "1148/1148 [==============================] - 1s 692us/step - loss: 4.0493 - acc: 0.0866 - val_loss: 5.0255 - val_acc: 0.0883\n",
      "Epoch 13/500\n",
      "1148/1148 [==============================] - 1s 694us/step - loss: 4.0096 - acc: 0.0912 - val_loss: 5.0122 - val_acc: 0.0868\n",
      "Epoch 14/500\n",
      "1148/1148 [==============================] - 1s 692us/step - loss: 3.9770 - acc: 0.0838 - val_loss: 5.0107 - val_acc: 0.0865\n",
      "Epoch 15/500\n",
      "1148/1148 [==============================] - 1s 710us/step - loss: 3.9478 - acc: 0.0919 - val_loss: 5.0271 - val_acc: 0.0871\n",
      "Epoch 16/500\n",
      "1148/1148 [==============================] - 1s 714us/step - loss: 3.9153 - acc: 0.0918 - val_loss: 5.0219 - val_acc: 0.0880\n",
      "Epoch 17/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 3.8783 - acc: 0.0926 - val_loss: 5.0016 - val_acc: 0.0871\n",
      "Epoch 18/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 3.8374 - acc: 0.0932 - val_loss: 4.9819 - val_acc: 0.0848\n",
      "Epoch 19/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 3.7933 - acc: 0.0942 - val_loss: 4.9617 - val_acc: 0.0880\n",
      "Epoch 20/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 3.7554 - acc: 0.0965 - val_loss: 4.9512 - val_acc: 0.0854\n",
      "Epoch 21/500\n",
      "1148/1148 [==============================] - 1s 687us/step - loss: 3.7170 - acc: 0.0984 - val_loss: 4.9628 - val_acc: 0.0871\n",
      "Epoch 22/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 3.6824 - acc: 0.1014 - val_loss: 4.9654 - val_acc: 0.0836\n",
      "Epoch 23/500\n",
      "1148/1148 [==============================] - 1s 698us/step - loss: 3.6495 - acc: 0.1069 - val_loss: 4.9738 - val_acc: 0.0862\n",
      "Epoch 24/500\n",
      "1148/1148 [==============================] - 1s 689us/step - loss: 3.6175 - acc: 0.1130 - val_loss: 4.9873 - val_acc: 0.0880\n",
      "Epoch 25/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 3.5894 - acc: 0.1162 - val_loss: 4.9911 - val_acc: 0.0906\n",
      "Epoch 26/500\n",
      "1148/1148 [==============================] - 1s 672us/step - loss: 3.5599 - acc: 0.1201 - val_loss: 4.9901 - val_acc: 0.0871\n",
      "Epoch 27/500\n",
      "1148/1148 [==============================] - 1s 674us/step - loss: 3.5320 - acc: 0.1242 - val_loss: 4.9973 - val_acc: 0.0923\n",
      "Epoch 28/500\n",
      "1148/1148 [==============================] - 1s 668us/step - loss: 3.5044 - acc: 0.1226 - val_loss: 5.0065 - val_acc: 0.0891\n",
      "Epoch 29/500\n",
      "1148/1148 [==============================] - 1s 665us/step - loss: 3.4736 - acc: 0.1287 - val_loss: 5.0095 - val_acc: 0.0920\n",
      "Epoch 30/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 3.4429 - acc: 0.1273 - val_loss: 5.0175 - val_acc: 0.0888\n",
      "Epoch 31/500\n",
      "1148/1148 [==============================] - 1s 722us/step - loss: 3.4121 - acc: 0.1312 - val_loss: 5.0182 - val_acc: 0.0903\n",
      "Epoch 32/500\n",
      "1148/1148 [==============================] - 1s 687us/step - loss: 3.3813 - acc: 0.1325 - val_loss: 5.0318 - val_acc: 0.0891\n",
      "Epoch 33/500\n",
      "1148/1148 [==============================] - 1s 664us/step - loss: 3.3528 - acc: 0.1344 - val_loss: 5.0380 - val_acc: 0.0917\n",
      "Epoch 34/500\n",
      "1148/1148 [==============================] - 1s 671us/step - loss: 3.3223 - acc: 0.1402 - val_loss: 5.0438 - val_acc: 0.0940\n",
      "Epoch 35/500\n",
      "1148/1148 [==============================] - 1s 673us/step - loss: 3.2952 - acc: 0.1389 - val_loss: 5.0560 - val_acc: 0.0900\n",
      "Epoch 36/500\n",
      "1148/1148 [==============================] - 1s 668us/step - loss: 3.2643 - acc: 0.1419 - val_loss: 5.0668 - val_acc: 0.0932\n",
      "Epoch 37/500\n",
      "1148/1148 [==============================] - 1s 670us/step - loss: 3.2359 - acc: 0.1473 - val_loss: 5.0702 - val_acc: 0.0885\n",
      "Epoch 38/500\n",
      "1148/1148 [==============================] - 1s 665us/step - loss: 3.2088 - acc: 0.1488 - val_loss: 5.0744 - val_acc: 0.0923\n",
      "Epoch 39/500\n",
      "1148/1148 [==============================] - 1s 671us/step - loss: 3.1815 - acc: 0.1512 - val_loss: 5.0867 - val_acc: 0.0923\n",
      "Epoch 40/500\n",
      "1148/1148 [==============================] - 1s 674us/step - loss: 3.1539 - acc: 0.1528 - val_loss: 5.0910 - val_acc: 0.0929\n",
      "Epoch 41/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 3.1249 - acc: 0.1577 - val_loss: 5.0959 - val_acc: 0.0865\n",
      "Epoch 42/500\n",
      "1148/1148 [==============================] - 1s 675us/step - loss: 3.0972 - acc: 0.1582 - val_loss: 5.1092 - val_acc: 0.0894\n",
      "Epoch 43/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 3.0711 - acc: 0.1609 - val_loss: 5.1171 - val_acc: 0.0909\n",
      "Epoch 44/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 3.0459 - acc: 0.1677 - val_loss: 5.1280 - val_acc: 0.0903\n",
      "Epoch 45/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 3.0203 - acc: 0.1712 - val_loss: 5.1311 - val_acc: 0.0911\n",
      "Epoch 46/500\n",
      "1148/1148 [==============================] - 1s 673us/step - loss: 2.9968 - acc: 0.1728 - val_loss: 5.1452 - val_acc: 0.0885\n",
      "Epoch 47/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 2.9727 - acc: 0.1753 - val_loss: 5.1517 - val_acc: 0.0868\n",
      "Epoch 48/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 2.9468 - acc: 0.1792 - val_loss: 5.1601 - val_acc: 0.0839\n",
      "Epoch 49/500\n",
      "1148/1148 [==============================] - 1s 674us/step - loss: 2.9247 - acc: 0.1808 - val_loss: 5.1676 - val_acc: 0.0836\n",
      "Epoch 50/500\n",
      "1148/1148 [==============================] - 1s 674us/step - loss: 2.9016 - acc: 0.1834 - val_loss: 5.1783 - val_acc: 0.0822\n",
      "Epoch 51/500\n",
      "1148/1148 [==============================] - 1s 672us/step - loss: 2.8807 - acc: 0.1852 - val_loss: 5.1919 - val_acc: 0.0839\n",
      "Epoch 52/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 2.8584 - acc: 0.1891 - val_loss: 5.2006 - val_acc: 0.0842\n",
      "Epoch 53/500\n",
      "1148/1148 [==============================] - 1s 708us/step - loss: 2.8371 - acc: 0.1903 - val_loss: 5.2083 - val_acc: 0.0848\n",
      "Epoch 54/500\n",
      "1148/1148 [==============================] - 1s 689us/step - loss: 2.8157 - acc: 0.1938 - val_loss: 5.2165 - val_acc: 0.0825\n",
      "Epoch 55/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 2.7961 - acc: 0.1965 - val_loss: 5.2282 - val_acc: 0.0833\n",
      "Epoch 56/500\n",
      "1148/1148 [==============================] - 1s 709us/step - loss: 2.7778 - acc: 0.1980 - val_loss: 5.2368 - val_acc: 0.0828\n",
      "Epoch 57/500\n",
      "1148/1148 [==============================] - 1s 691us/step - loss: 2.7581 - acc: 0.2013 - val_loss: 5.2482 - val_acc: 0.0839\n",
      "Epoch 58/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 2.7387 - acc: 0.2017 - val_loss: 5.2551 - val_acc: 0.0839\n",
      "Epoch 59/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 2.7212 - acc: 0.2051 - val_loss: 5.2673 - val_acc: 0.0825\n",
      "Epoch 60/500\n",
      "1148/1148 [==============================] - 1s 673us/step - loss: 2.7045 - acc: 0.2054 - val_loss: 5.2806 - val_acc: 0.0828\n",
      "Epoch 61/500\n",
      "1148/1148 [==============================] - 1s 676us/step - loss: 2.6877 - acc: 0.2091 - val_loss: 5.2835 - val_acc: 0.0813\n",
      "Epoch 62/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 2.6713 - acc: 0.2105 - val_loss: 5.2925 - val_acc: 0.0828\n",
      "Epoch 63/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 2.6549 - acc: 0.2123 - val_loss: 5.3055 - val_acc: 0.0822\n",
      "Epoch 64/500\n",
      "1148/1148 [==============================] - 1s 705us/step - loss: 2.6396 - acc: 0.2123 - val_loss: 5.3101 - val_acc: 0.0845\n",
      "Epoch 65/500\n",
      "1148/1148 [==============================] - 1s 702us/step - loss: 2.6244 - acc: 0.2155 - val_loss: 5.3261 - val_acc: 0.0828\n",
      "Epoch 66/500\n",
      "1148/1148 [==============================] - 1s 687us/step - loss: 2.6104 - acc: 0.2174 - val_loss: 5.3341 - val_acc: 0.0839\n",
      "Epoch 67/500\n",
      "1148/1148 [==============================] - 1s 689us/step - loss: 2.5933 - acc: 0.2197 - val_loss: 5.3485 - val_acc: 0.0822\n",
      "Epoch 68/500\n",
      "1148/1148 [==============================] - 1s 702us/step - loss: 2.5784 - acc: 0.2210 - val_loss: 5.3598 - val_acc: 0.0810\n",
      "Epoch 69/500\n",
      "1148/1148 [==============================] - 1s 721us/step - loss: 2.5644 - acc: 0.2233 - val_loss: 5.3667 - val_acc: 0.0810\n",
      "Epoch 70/500\n",
      "1148/1148 [==============================] - 1s 717us/step - loss: 2.5510 - acc: 0.2233 - val_loss: 5.3748 - val_acc: 0.0819\n",
      "Epoch 71/500\n",
      "1148/1148 [==============================] - 1s 710us/step - loss: 2.5366 - acc: 0.2252 - val_loss: 5.3935 - val_acc: 0.0810\n",
      "Epoch 72/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 2.5260 - acc: 0.2263 - val_loss: 5.3974 - val_acc: 0.0813\n",
      "Epoch 73/500\n",
      "1148/1148 [==============================] - 1s 688us/step - loss: 2.5137 - acc: 0.2270 - val_loss: 5.4067 - val_acc: 0.0802\n",
      "Epoch 74/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 2.5013 - acc: 0.2300 - val_loss: 5.4159 - val_acc: 0.0813\n",
      "Epoch 75/500\n",
      "1148/1148 [==============================] - 1s 687us/step - loss: 2.4882 - acc: 0.2310 - val_loss: 5.4217 - val_acc: 0.0804\n",
      "Epoch 76/500\n",
      "1148/1148 [==============================] - 1s 692us/step - loss: 2.4762 - acc: 0.2331 - val_loss: 5.4373 - val_acc: 0.0784\n",
      "Epoch 77/500\n",
      "1148/1148 [==============================] - 1s 691us/step - loss: 2.4631 - acc: 0.2337 - val_loss: 5.4406 - val_acc: 0.0790\n",
      "Epoch 78/500\n",
      "1148/1148 [==============================] - 1s 703us/step - loss: 2.4508 - acc: 0.2361 - val_loss: 5.4563 - val_acc: 0.0799\n",
      "Epoch 79/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 2.4372 - acc: 0.2366 - val_loss: 5.4597 - val_acc: 0.0790\n",
      "Epoch 80/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 2.4254 - acc: 0.2384 - val_loss: 5.4691 - val_acc: 0.0775\n",
      "Epoch 81/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 2.4147 - acc: 0.2394 - val_loss: 5.4734 - val_acc: 0.0781\n",
      "Epoch 82/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 2.4018 - acc: 0.2414 - val_loss: 5.4889 - val_acc: 0.0784\n",
      "Epoch 83/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 2.3905 - acc: 0.2427 - val_loss: 5.5004 - val_acc: 0.0767\n",
      "Epoch 84/500\n",
      "1148/1148 [==============================] - 1s 700us/step - loss: 2.3796 - acc: 0.2433 - val_loss: 5.5098 - val_acc: 0.0775\n",
      "Epoch 85/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 2.3675 - acc: 0.2446 - val_loss: 5.5141 - val_acc: 0.0781\n",
      "Epoch 86/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 2.3575 - acc: 0.2451 - val_loss: 5.5137 - val_acc: 0.0770\n",
      "Epoch 87/500\n",
      "1148/1148 [==============================] - 1s 674us/step - loss: 2.3478 - acc: 0.2471 - val_loss: 5.5276 - val_acc: 0.0770\n",
      "Epoch 88/500\n",
      "1148/1148 [==============================] - 1s 671us/step - loss: 2.3377 - acc: 0.2462 - val_loss: 5.5374 - val_acc: 0.0767\n",
      "Epoch 89/500\n",
      "1148/1148 [==============================] - 1s 669us/step - loss: 2.3280 - acc: 0.2485 - val_loss: 5.5511 - val_acc: 0.0755\n",
      "Epoch 90/500\n",
      "1148/1148 [==============================] - 1s 676us/step - loss: 2.3166 - acc: 0.2505 - val_loss: 5.5593 - val_acc: 0.0775\n",
      "Epoch 91/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 2.3050 - acc: 0.2511 - val_loss: 5.5685 - val_acc: 0.0773\n",
      "Epoch 92/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 2.2966 - acc: 0.2519 - val_loss: 5.5758 - val_acc: 0.0773\n",
      "Epoch 93/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 2.2854 - acc: 0.2535 - val_loss: 5.5924 - val_acc: 0.0749\n",
      "Epoch 94/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 2.2745 - acc: 0.2552 - val_loss: 5.5959 - val_acc: 0.0775\n",
      "Epoch 95/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 2.2659 - acc: 0.2562 - val_loss: 5.6066 - val_acc: 0.0752\n",
      "Epoch 96/500\n",
      "1148/1148 [==============================] - 1s 668us/step - loss: 2.2567 - acc: 0.2565 - val_loss: 5.6203 - val_acc: 0.0752\n",
      "Epoch 97/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 2.2475 - acc: 0.2586 - val_loss: 5.6272 - val_acc: 0.0749\n",
      "Epoch 98/500\n",
      "1148/1148 [==============================] - 1s 669us/step - loss: 2.2376 - acc: 0.2593 - val_loss: 5.6255 - val_acc: 0.0744\n",
      "Epoch 99/500\n",
      "1148/1148 [==============================] - 1s 667us/step - loss: 2.2280 - acc: 0.2611 - val_loss: 5.6387 - val_acc: 0.0767\n",
      "Epoch 100/500\n",
      "1148/1148 [==============================] - 1s 672us/step - loss: 2.2184 - acc: 0.2618 - val_loss: 5.6485 - val_acc: 0.0752\n",
      "Epoch 101/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 2.2073 - acc: 0.2640 - val_loss: 5.6511 - val_acc: 0.0752\n",
      "Epoch 102/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 2.2000 - acc: 0.2652 - val_loss: 5.6633 - val_acc: 0.0749\n",
      "Epoch 103/500\n",
      "1148/1148 [==============================] - ETA: 0s - loss: 2.1948 - acc: 0.264 - 1s 683us/step - loss: 2.1922 - acc: 0.2644 - val_loss: 5.6678 - val_acc: 0.0723\n",
      "Epoch 104/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 2.1833 - acc: 0.2671 - val_loss: 5.6781 - val_acc: 0.0764\n",
      "Epoch 105/500\n",
      "1148/1148 [==============================] - 1s 694us/step - loss: 2.1748 - acc: 0.2671 - val_loss: 5.6889 - val_acc: 0.0749\n",
      "Epoch 106/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 2.1642 - acc: 0.2673 - val_loss: 5.6900 - val_acc: 0.0770\n",
      "Epoch 107/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 2.1574 - acc: 0.2688 - val_loss: 5.7023 - val_acc: 0.0755\n",
      "Epoch 108/500\n",
      "1148/1148 [==============================] - 1s 715us/step - loss: 2.1504 - acc: 0.2701 - val_loss: 5.7057 - val_acc: 0.0755\n",
      "Epoch 109/500\n",
      "1148/1148 [==============================] - 1s 695us/step - loss: 2.1413 - acc: 0.2716 - val_loss: 5.7139 - val_acc: 0.0726\n",
      "Epoch 110/500\n",
      "1148/1148 [==============================] - 1s 704us/step - loss: 2.1336 - acc: 0.2721 - val_loss: 5.7167 - val_acc: 0.0735\n",
      "Epoch 111/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 2.1264 - acc: 0.2724 - val_loss: 5.7243 - val_acc: 0.0744\n",
      "Epoch 112/500\n",
      "1148/1148 [==============================] - 1s 712us/step - loss: 2.1181 - acc: 0.2729 - val_loss: 5.7378 - val_acc: 0.0747\n",
      "Epoch 113/500\n",
      "1148/1148 [==============================] - 1s 735us/step - loss: 2.1125 - acc: 0.2750 - val_loss: 5.7393 - val_acc: 0.0738\n",
      "Epoch 114/500\n",
      "1148/1148 [==============================] - 1s 731us/step - loss: 2.1077 - acc: 0.2752 - val_loss: 5.7412 - val_acc: 0.0744\n",
      "Epoch 115/500\n",
      "1148/1148 [==============================] - 1s 705us/step - loss: 2.0981 - acc: 0.2759 - val_loss: 5.7560 - val_acc: 0.0755\n",
      "Epoch 116/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 2.0902 - acc: 0.2782 - val_loss: 5.7687 - val_acc: 0.0741\n",
      "Epoch 117/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 2.0826 - acc: 0.2789 - val_loss: 5.7746 - val_acc: 0.0741\n",
      "Epoch 118/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 2.0766 - acc: 0.2790 - val_loss: 5.7767 - val_acc: 0.0749\n",
      "Epoch 119/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148/1148 [==============================] - 1s 694us/step - loss: 2.0690 - acc: 0.2808 - val_loss: 5.7822 - val_acc: 0.0738\n",
      "Epoch 120/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 2.0608 - acc: 0.2807 - val_loss: 5.7886 - val_acc: 0.0735\n",
      "Epoch 121/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 2.0525 - acc: 0.2832 - val_loss: 5.7983 - val_acc: 0.0735\n",
      "Epoch 122/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 2.0431 - acc: 0.2848 - val_loss: 5.7984 - val_acc: 0.0755\n",
      "Epoch 123/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 2.0379 - acc: 0.2848 - val_loss: 5.8000 - val_acc: 0.0738\n",
      "Epoch 124/500\n",
      "1148/1148 [==============================] - 1s 696us/step - loss: 2.0304 - acc: 0.2862 - val_loss: 5.8205 - val_acc: 0.0738\n",
      "Epoch 125/500\n",
      "1148/1148 [==============================] - 1s 699us/step - loss: 2.0238 - acc: 0.2865 - val_loss: 5.8182 - val_acc: 0.0741\n",
      "Epoch 126/500\n",
      "1148/1148 [==============================] - 1s 692us/step - loss: 2.0151 - acc: 0.2883 - val_loss: 5.8341 - val_acc: 0.0732\n",
      "Epoch 127/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 2.0081 - acc: 0.2902 - val_loss: 5.8373 - val_acc: 0.0732\n",
      "Epoch 128/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 2.0017 - acc: 0.2899 - val_loss: 5.8465 - val_acc: 0.0744\n",
      "Epoch 129/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.9943 - acc: 0.2928 - val_loss: 5.8524 - val_acc: 0.0723\n",
      "Epoch 130/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 1.9872 - acc: 0.2915 - val_loss: 5.8604 - val_acc: 0.0712\n",
      "Epoch 131/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.9832 - acc: 0.2951 - val_loss: 5.8644 - val_acc: 0.0735\n",
      "Epoch 132/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.9752 - acc: 0.2955 - val_loss: 5.8819 - val_acc: 0.0726\n",
      "Epoch 133/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.9688 - acc: 0.2957 - val_loss: 5.8824 - val_acc: 0.0747\n",
      "Epoch 134/500\n",
      "1148/1148 [==============================] - 1s 675us/step - loss: 1.9617 - acc: 0.2970 - val_loss: 5.8971 - val_acc: 0.0732\n",
      "Epoch 135/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.9574 - acc: 0.2952 - val_loss: 5.9015 - val_acc: 0.0718\n",
      "Epoch 136/500\n",
      "1148/1148 [==============================] - 1s 704us/step - loss: 1.9495 - acc: 0.2978 - val_loss: 5.8982 - val_acc: 0.0712\n",
      "Epoch 137/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 1.9440 - acc: 0.3000 - val_loss: 5.9148 - val_acc: 0.0726\n",
      "Epoch 138/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 1.9380 - acc: 0.2999 - val_loss: 5.9186 - val_acc: 0.0723\n",
      "Epoch 139/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.9322 - acc: 0.3020 - val_loss: 5.9204 - val_acc: 0.0709\n",
      "Epoch 140/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.9257 - acc: 0.3032 - val_loss: 5.9304 - val_acc: 0.0735\n",
      "Epoch 141/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.9201 - acc: 0.3026 - val_loss: 5.9372 - val_acc: 0.0726\n",
      "Epoch 142/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.9152 - acc: 0.3048 - val_loss: 5.9419 - val_acc: 0.0767\n",
      "Epoch 143/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.9099 - acc: 0.3061 - val_loss: 5.9466 - val_acc: 0.0715\n",
      "Epoch 144/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 1.9041 - acc: 0.3058 - val_loss: 5.9458 - val_acc: 0.0720\n",
      "Epoch 145/500\n",
      "1148/1148 [==============================] - 1s 704us/step - loss: 1.8993 - acc: 0.3051 - val_loss: 5.9551 - val_acc: 0.0723\n",
      "Epoch 146/500\n",
      "1148/1148 [==============================] - 1s 692us/step - loss: 1.8962 - acc: 0.3062 - val_loss: 5.9690 - val_acc: 0.0709\n",
      "Epoch 147/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 1.8891 - acc: 0.3082 - val_loss: 5.9704 - val_acc: 0.0706\n",
      "Epoch 148/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.8846 - acc: 0.3102 - val_loss: 5.9664 - val_acc: 0.0732\n",
      "Epoch 149/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.8778 - acc: 0.3112 - val_loss: 5.9926 - val_acc: 0.0718\n",
      "Epoch 150/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.8715 - acc: 0.3111 - val_loss: 5.9858 - val_acc: 0.0706\n",
      "Epoch 151/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.8678 - acc: 0.3104 - val_loss: 5.9857 - val_acc: 0.0700\n",
      "Epoch 152/500\n",
      "1148/1148 [==============================] - 1s 687us/step - loss: 1.8623 - acc: 0.3117 - val_loss: 6.0074 - val_acc: 0.0709\n",
      "Epoch 153/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.8579 - acc: 0.3142 - val_loss: 5.9954 - val_acc: 0.0686\n",
      "Epoch 154/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.8543 - acc: 0.3152 - val_loss: 6.0122 - val_acc: 0.0689\n",
      "Epoch 155/500\n",
      "1148/1148 [==============================] - 1s 701us/step - loss: 1.8506 - acc: 0.3153 - val_loss: 6.0237 - val_acc: 0.0694\n",
      "Epoch 156/500\n",
      "1148/1148 [==============================] - 1s 716us/step - loss: 1.8458 - acc: 0.3177 - val_loss: 6.0256 - val_acc: 0.0706\n",
      "Epoch 157/500\n",
      "1148/1148 [==============================] - 1s 701us/step - loss: 1.8381 - acc: 0.3191 - val_loss: 6.0272 - val_acc: 0.0723\n",
      "Epoch 158/500\n",
      "1148/1148 [==============================] - 1s 698us/step - loss: 1.8329 - acc: 0.3179 - val_loss: 6.0394 - val_acc: 0.0715\n",
      "Epoch 159/500\n",
      "1148/1148 [==============================] - 1s 694us/step - loss: 1.8264 - acc: 0.3212 - val_loss: 6.0493 - val_acc: 0.0732\n",
      "Epoch 160/500\n",
      "1148/1148 [==============================] - 1s 703us/step - loss: 1.8218 - acc: 0.3204 - val_loss: 6.0356 - val_acc: 0.0706\n",
      "Epoch 161/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 1.8155 - acc: 0.3212 - val_loss: 6.0552 - val_acc: 0.0706\n",
      "Epoch 162/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.8136 - acc: 0.3205 - val_loss: 6.0434 - val_acc: 0.0697\n",
      "Epoch 163/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.8090 - acc: 0.3217 - val_loss: 6.0520 - val_acc: 0.0715\n",
      "Epoch 164/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 1.8034 - acc: 0.3230 - val_loss: 6.0610 - val_acc: 0.0709\n",
      "Epoch 165/500\n",
      "1148/1148 [==============================] - 1s 718us/step - loss: 1.7998 - acc: 0.3220 - val_loss: 6.0648 - val_acc: 0.0692\n",
      "Epoch 166/500\n",
      "1148/1148 [==============================] - 1s 720us/step - loss: 1.7957 - acc: 0.3242 - val_loss: 6.0862 - val_acc: 0.0703\n",
      "Epoch 167/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.7895 - acc: 0.3268 - val_loss: 6.0908 - val_acc: 0.0703\n",
      "Epoch 168/500\n",
      "1148/1148 [==============================] - 1s 696us/step - loss: 1.7856 - acc: 0.3257 - val_loss: 6.0913 - val_acc: 0.0697\n",
      "Epoch 169/500\n",
      "1148/1148 [==============================] - 1s 696us/step - loss: 1.7806 - acc: 0.3260 - val_loss: 6.1048 - val_acc: 0.0686\n",
      "Epoch 170/500\n",
      "1148/1148 [==============================] - 1s 702us/step - loss: 1.7769 - acc: 0.3269 - val_loss: 6.1115 - val_acc: 0.0674\n",
      "Epoch 171/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 1.7749 - acc: 0.3256 - val_loss: 6.1139 - val_acc: 0.0674\n",
      "Epoch 172/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.7683 - acc: 0.3291 - val_loss: 6.1162 - val_acc: 0.0703\n",
      "Epoch 173/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.7620 - acc: 0.3297 - val_loss: 6.1336 - val_acc: 0.0680\n",
      "Epoch 174/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 1.7580 - acc: 0.3295 - val_loss: 6.1340 - val_acc: 0.0668\n",
      "Epoch 175/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.7548 - acc: 0.3308 - val_loss: 6.1463 - val_acc: 0.0671\n",
      "Epoch 176/500\n",
      "1148/1148 [==============================] - 1s 676us/step - loss: 1.7513 - acc: 0.3322 - val_loss: 6.1384 - val_acc: 0.0674\n",
      "Epoch 177/500\n",
      "1148/1148 [==============================] - 1s 692us/step - loss: 1.7452 - acc: 0.3336 - val_loss: 6.1586 - val_acc: 0.0677\n",
      "Epoch 178/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148/1148 [==============================] - 1s 697us/step - loss: 1.7417 - acc: 0.3340 - val_loss: 6.1597 - val_acc: 0.0668\n",
      "Epoch 179/500\n",
      "1148/1148 [==============================] - 1s 756us/step - loss: 1.7368 - acc: 0.3330 - val_loss: 6.1612 - val_acc: 0.0686\n",
      "Epoch 180/500\n",
      "1148/1148 [==============================] - 1s 755us/step - loss: 1.7340 - acc: 0.3336 - val_loss: 6.1653 - val_acc: 0.0668\n",
      "Epoch 181/500\n",
      "1148/1148 [==============================] - 1s 711us/step - loss: 1.7322 - acc: 0.3340 - val_loss: 6.1761 - val_acc: 0.0668\n",
      "Epoch 182/500\n",
      "1148/1148 [==============================] - 1s 688us/step - loss: 1.7282 - acc: 0.3349 - val_loss: 6.1639 - val_acc: 0.0694\n",
      "Epoch 183/500\n",
      "1148/1148 [==============================] - 1s 703us/step - loss: 1.7228 - acc: 0.3372 - val_loss: 6.1885 - val_acc: 0.0674\n",
      "Epoch 184/500\n",
      "1148/1148 [==============================] - 1s 710us/step - loss: 1.7200 - acc: 0.3351 - val_loss: 6.1871 - val_acc: 0.0683\n",
      "Epoch 185/500\n",
      "1148/1148 [==============================] - 1s 704us/step - loss: 1.7147 - acc: 0.3363 - val_loss: 6.2002 - val_acc: 0.0683\n",
      "Epoch 186/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 1.7098 - acc: 0.3378 - val_loss: 6.1958 - val_acc: 0.0651\n",
      "Epoch 187/500\n",
      "1148/1148 [==============================] - 1s 694us/step - loss: 1.7051 - acc: 0.3391 - val_loss: 6.1957 - val_acc: 0.0639\n",
      "Epoch 188/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.7016 - acc: 0.3383 - val_loss: 6.2142 - val_acc: 0.0674\n",
      "Epoch 189/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.6992 - acc: 0.3390 - val_loss: 6.1955 - val_acc: 0.0680\n",
      "Epoch 190/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 1.6952 - acc: 0.3401 - val_loss: 6.2095 - val_acc: 0.0668\n",
      "Epoch 191/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.6921 - acc: 0.3401 - val_loss: 6.2094 - val_acc: 0.0692\n",
      "Epoch 192/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.6870 - acc: 0.3420 - val_loss: 6.2276 - val_acc: 0.0715\n",
      "Epoch 193/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.6823 - acc: 0.3416 - val_loss: 6.2240 - val_acc: 0.0674\n",
      "Epoch 194/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.6797 - acc: 0.3432 - val_loss: 6.2454 - val_acc: 0.0668\n",
      "Epoch 195/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.6751 - acc: 0.3430 - val_loss: 6.2431 - val_acc: 0.0671\n",
      "Epoch 196/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.6735 - acc: 0.3443 - val_loss: 6.2410 - val_acc: 0.0720\n",
      "Epoch 197/500\n",
      "1148/1148 [==============================] - 1s 688us/step - loss: 1.6694 - acc: 0.3447 - val_loss: 6.2572 - val_acc: 0.0697\n",
      "Epoch 198/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.6628 - acc: 0.3453 - val_loss: 6.2508 - val_acc: 0.0697\n",
      "Epoch 199/500\n",
      "1148/1148 [==============================] - 1s 726us/step - loss: 1.6592 - acc: 0.3460 - val_loss: 6.2550 - val_acc: 0.0680\n",
      "Epoch 200/500\n",
      "1148/1148 [==============================] - 1s 731us/step - loss: 1.6549 - acc: 0.3479 - val_loss: 6.2598 - val_acc: 0.0692\n",
      "Epoch 201/500\n",
      "1148/1148 [==============================] - 1s 719us/step - loss: 1.6526 - acc: 0.3497 - val_loss: 6.2905 - val_acc: 0.0666\n",
      "Epoch 202/500\n",
      "1148/1148 [==============================] - 1s 713us/step - loss: 1.6486 - acc: 0.3484 - val_loss: 6.2743 - val_acc: 0.0674\n",
      "Epoch 203/500\n",
      "1148/1148 [==============================] - 1s 710us/step - loss: 1.6441 - acc: 0.3497 - val_loss: 6.2850 - val_acc: 0.0654\n",
      "Epoch 204/500\n",
      "1148/1148 [==============================] - 1s 730us/step - loss: 1.6396 - acc: 0.3507 - val_loss: 6.2914 - val_acc: 0.0666\n",
      "Epoch 205/500\n",
      "1148/1148 [==============================] - 1s 728us/step - loss: 1.6355 - acc: 0.3528 - val_loss: 6.3074 - val_acc: 0.0648\n",
      "Epoch 206/500\n",
      "1148/1148 [==============================] - 1s 717us/step - loss: 1.6319 - acc: 0.3512 - val_loss: 6.3048 - val_acc: 0.0631\n",
      "Epoch 207/500\n",
      "1148/1148 [==============================] - 1s 714us/step - loss: 1.6285 - acc: 0.3519 - val_loss: 6.3166 - val_acc: 0.0648\n",
      "Epoch 208/500\n",
      "1148/1148 [==============================] - 1s 711us/step - loss: 1.6255 - acc: 0.3527 - val_loss: 6.3095 - val_acc: 0.0637\n",
      "Epoch 209/500\n",
      "1148/1148 [==============================] - 1s 729us/step - loss: 1.6222 - acc: 0.3524 - val_loss: 6.3270 - val_acc: 0.0651\n",
      "Epoch 210/500\n",
      "1148/1148 [==============================] - 1s 738us/step - loss: 1.6209 - acc: 0.3529 - val_loss: 6.3326 - val_acc: 0.0645\n",
      "Epoch 211/500\n",
      "1148/1148 [==============================] - 1s 723us/step - loss: 1.6169 - acc: 0.3552 - val_loss: 6.3213 - val_acc: 0.0654\n",
      "Epoch 212/500\n",
      "1148/1148 [==============================] - 1s 704us/step - loss: 1.6170 - acc: 0.3535 - val_loss: 6.3452 - val_acc: 0.0639\n",
      "Epoch 213/500\n",
      "1148/1148 [==============================] - 1s 703us/step - loss: 1.6144 - acc: 0.3548 - val_loss: 6.3321 - val_acc: 0.0660\n",
      "Epoch 214/500\n",
      "1148/1148 [==============================] - 1s 698us/step - loss: 1.6080 - acc: 0.3575 - val_loss: 6.3434 - val_acc: 0.0619\n",
      "Epoch 215/500\n",
      "1148/1148 [==============================] - 1s 694us/step - loss: 1.6074 - acc: 0.3548 - val_loss: 6.3437 - val_acc: 0.0639\n",
      "Epoch 216/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.6044 - acc: 0.3558 - val_loss: 6.3441 - val_acc: 0.0642\n",
      "Epoch 217/500\n",
      "1148/1148 [==============================] - 1s 707us/step - loss: 1.5993 - acc: 0.3577 - val_loss: 6.3641 - val_acc: 0.0645\n",
      "Epoch 218/500\n",
      "1148/1148 [==============================] - 1s 692us/step - loss: 1.5945 - acc: 0.3584 - val_loss: 6.3483 - val_acc: 0.0634\n",
      "Epoch 219/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.5882 - acc: 0.3600 - val_loss: 6.3654 - val_acc: 0.0628\n",
      "Epoch 220/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.5861 - acc: 0.3612 - val_loss: 6.3643 - val_acc: 0.0625\n",
      "Epoch 221/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.5818 - acc: 0.3622 - val_loss: 6.3775 - val_acc: 0.0616\n",
      "Epoch 222/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.5802 - acc: 0.3604 - val_loss: 6.3757 - val_acc: 0.0622\n",
      "Epoch 223/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.5760 - acc: 0.3622 - val_loss: 6.3895 - val_acc: 0.0631\n",
      "Epoch 224/500\n",
      "1148/1148 [==============================] - 1s 700us/step - loss: 1.5731 - acc: 0.3611 - val_loss: 6.4068 - val_acc: 0.0622\n",
      "Epoch 225/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 1.5688 - acc: 0.3640 - val_loss: 6.3924 - val_acc: 0.0619\n",
      "Epoch 226/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.5669 - acc: 0.3635 - val_loss: 6.4006 - val_acc: 0.0622\n",
      "Epoch 227/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.5649 - acc: 0.3644 - val_loss: 6.4172 - val_acc: 0.0622\n",
      "Epoch 228/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.5629 - acc: 0.3642 - val_loss: 6.4305 - val_acc: 0.0648\n",
      "Epoch 229/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 1.5585 - acc: 0.3643 - val_loss: 6.4199 - val_acc: 0.0625\n",
      "Epoch 230/500\n",
      "1148/1148 [==============================] - ETA: 0s - loss: 1.5543 - acc: 0.365 - 1s 680us/step - loss: 1.5571 - acc: 0.3661 - val_loss: 6.4134 - val_acc: 0.0619\n",
      "Epoch 231/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.5517 - acc: 0.3675 - val_loss: 6.4185 - val_acc: 0.0628\n",
      "Epoch 232/500\n",
      "1148/1148 [==============================] - ETA: 0s - loss: 1.5510 - acc: 0.367 - 1s 683us/step - loss: 1.5517 - acc: 0.3655 - val_loss: 6.4415 - val_acc: 0.0625\n",
      "Epoch 233/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.5479 - acc: 0.3660 - val_loss: 6.4361 - val_acc: 0.0637\n",
      "Epoch 234/500\n",
      "1148/1148 [==============================] - ETA: 0s - loss: 1.5435 - acc: 0.369 - 1s 680us/step - loss: 1.5462 - acc: 0.3675 - val_loss: 6.4437 - val_acc: 0.0642\n",
      "Epoch 235/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.5444 - acc: 0.3681 - val_loss: 6.4574 - val_acc: 0.0622\n",
      "Epoch 236/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.5416 - acc: 0.3677 - val_loss: 6.4677 - val_acc: 0.0616\n",
      "Epoch 237/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.5384 - acc: 0.3693 - val_loss: 6.4543 - val_acc: 0.0642\n",
      "Epoch 238/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.5463 - acc: 0.3667 - val_loss: 6.4621 - val_acc: 0.0605\n",
      "Epoch 239/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 1.5486 - acc: 0.3669 - val_loss: 6.4550 - val_acc: 0.0596\n",
      "Epoch 240/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.5471 - acc: 0.3659 - val_loss: 6.4619 - val_acc: 0.0611\n",
      "Epoch 241/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.5393 - acc: 0.3669 - val_loss: 6.4681 - val_acc: 0.0619\n",
      "Epoch 242/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.5322 - acc: 0.3690 - val_loss: 6.4932 - val_acc: 0.0605\n",
      "Epoch 243/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 1.5289 - acc: 0.3668 - val_loss: 6.4759 - val_acc: 0.0622\n",
      "Epoch 244/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.5288 - acc: 0.3669 - val_loss: 6.4904 - val_acc: 0.0619\n",
      "Epoch 245/500\n",
      "1148/1148 [==============================] - 1s 699us/step - loss: 1.5260 - acc: 0.3705 - val_loss: 6.4842 - val_acc: 0.0639\n",
      "Epoch 246/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.5238 - acc: 0.3708 - val_loss: 6.4957 - val_acc: 0.0616\n",
      "Epoch 247/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.5166 - acc: 0.3722 - val_loss: 6.4795 - val_acc: 0.0622\n",
      "Epoch 248/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 1.5127 - acc: 0.3725 - val_loss: 6.4979 - val_acc: 0.0599\n",
      "Epoch 249/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.5103 - acc: 0.3732 - val_loss: 6.5030 - val_acc: 0.0611\n",
      "Epoch 250/500\n",
      "1148/1148 [==============================] - 1s 689us/step - loss: 1.5065 - acc: 0.3726 - val_loss: 6.5098 - val_acc: 0.0622\n",
      "Epoch 251/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.5017 - acc: 0.3776 - val_loss: 6.5177 - val_acc: 0.0631\n",
      "Epoch 252/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.4982 - acc: 0.3766 - val_loss: 6.5114 - val_acc: 0.0616\n",
      "Epoch 253/500\n",
      "1148/1148 [==============================] - 1s 691us/step - loss: 1.4963 - acc: 0.3758 - val_loss: 6.5226 - val_acc: 0.0622\n",
      "Epoch 254/500\n",
      "1148/1148 [==============================] - 1s 702us/step - loss: 1.4912 - acc: 0.3777 - val_loss: 6.5205 - val_acc: 0.0628\n",
      "Epoch 255/500\n",
      "1148/1148 [==============================] - 1s 709us/step - loss: 1.4899 - acc: 0.3755 - val_loss: 6.5280 - val_acc: 0.0613\n",
      "Epoch 256/500\n",
      "1148/1148 [==============================] - 1s 691us/step - loss: 1.4879 - acc: 0.3770 - val_loss: 6.5322 - val_acc: 0.0619\n",
      "Epoch 257/500\n",
      "1148/1148 [==============================] - 1s 700us/step - loss: 1.4853 - acc: 0.3773 - val_loss: 6.5347 - val_acc: 0.0619\n",
      "Epoch 258/500\n",
      "1148/1148 [==============================] - 1s 724us/step - loss: 1.4836 - acc: 0.3789 - val_loss: 6.5304 - val_acc: 0.0616\n",
      "Epoch 259/500\n",
      "1148/1148 [==============================] - 1s 718us/step - loss: 1.4832 - acc: 0.3799 - val_loss: 6.5364 - val_acc: 0.0637\n",
      "Epoch 260/500\n",
      "1148/1148 [==============================] - 1s 714us/step - loss: 1.4782 - acc: 0.3814 - val_loss: 6.5517 - val_acc: 0.0628\n",
      "Epoch 261/500\n",
      "1148/1148 [==============================] - 1s 704us/step - loss: 1.4759 - acc: 0.3792 - val_loss: 6.5611 - val_acc: 0.0619\n",
      "Epoch 262/500\n",
      "1148/1148 [==============================] - 1s 706us/step - loss: 1.4745 - acc: 0.3801 - val_loss: 6.5628 - val_acc: 0.0634\n",
      "Epoch 263/500\n",
      "1148/1148 [==============================] - 1s 698us/step - loss: 1.4706 - acc: 0.3812 - val_loss: 6.5753 - val_acc: 0.0622\n",
      "Epoch 264/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.4678 - acc: 0.3821 - val_loss: 6.5668 - val_acc: 0.0619\n",
      "Epoch 265/500\n",
      "1148/1148 [==============================] - 1s 701us/step - loss: 1.4663 - acc: 0.3811 - val_loss: 6.5779 - val_acc: 0.0602\n",
      "Epoch 266/500\n",
      "1148/1148 [==============================] - 1s 701us/step - loss: 1.4665 - acc: 0.3834 - val_loss: 6.5627 - val_acc: 0.0634\n",
      "Epoch 267/500\n",
      "1148/1148 [==============================] - 1s 691us/step - loss: 1.4607 - acc: 0.3826 - val_loss: 6.5826 - val_acc: 0.0634\n",
      "Epoch 268/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.4614 - acc: 0.3807 - val_loss: 6.5738 - val_acc: 0.0622\n",
      "Epoch 269/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 1.4622 - acc: 0.3828 - val_loss: 6.5965 - val_acc: 0.0648\n",
      "Epoch 270/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 1.4584 - acc: 0.3818 - val_loss: 6.5764 - val_acc: 0.0634\n",
      "Epoch 271/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 1.4560 - acc: 0.3826 - val_loss: 6.6011 - val_acc: 0.0622\n",
      "Epoch 272/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.4537 - acc: 0.3857 - val_loss: 6.6072 - val_acc: 0.0645\n",
      "Epoch 273/500\n",
      "1148/1148 [==============================] - 1s 692us/step - loss: 1.4521 - acc: 0.3841 - val_loss: 6.5987 - val_acc: 0.0622\n",
      "Epoch 274/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.4505 - acc: 0.3834 - val_loss: 6.6058 - val_acc: 0.0634\n",
      "Epoch 275/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.4497 - acc: 0.3849 - val_loss: 6.6110 - val_acc: 0.0634\n",
      "Epoch 276/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.4496 - acc: 0.3848 - val_loss: 6.6106 - val_acc: 0.0611\n",
      "Epoch 277/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.4488 - acc: 0.3842 - val_loss: 6.6209 - val_acc: 0.0637\n",
      "Epoch 278/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 1.4495 - acc: 0.3844 - val_loss: 6.6252 - val_acc: 0.0625\n",
      "Epoch 279/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.4446 - acc: 0.3878 - val_loss: 6.6291 - val_acc: 0.0637\n",
      "Epoch 280/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 1.4406 - acc: 0.3878 - val_loss: 6.6371 - val_acc: 0.0637\n",
      "Epoch 281/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.4375 - acc: 0.3870 - val_loss: 6.6253 - val_acc: 0.0642\n",
      "Epoch 282/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.4341 - acc: 0.3875 - val_loss: 6.6480 - val_acc: 0.0634\n",
      "Epoch 283/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.4293 - acc: 0.3889 - val_loss: 6.6530 - val_acc: 0.0639\n",
      "Epoch 284/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.4265 - acc: 0.3913 - val_loss: 6.6486 - val_acc: 0.0651\n",
      "Epoch 285/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 1.4228 - acc: 0.3910 - val_loss: 6.6544 - val_acc: 0.0654\n",
      "Epoch 286/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 1.4217 - acc: 0.3923 - val_loss: 6.6592 - val_acc: 0.0628\n",
      "Epoch 287/500\n",
      "1148/1148 [==============================] - 1s 687us/step - loss: 1.4217 - acc: 0.3920 - val_loss: 6.6740 - val_acc: 0.0639\n",
      "Epoch 288/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.4209 - acc: 0.3913 - val_loss: 6.6687 - val_acc: 0.0622\n",
      "Epoch 289/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.4218 - acc: 0.3905 - val_loss: 6.6744 - val_acc: 0.0631\n",
      "Epoch 290/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.4189 - acc: 0.3910 - val_loss: 6.6745 - val_acc: 0.0613\n",
      "Epoch 291/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.4182 - acc: 0.3903 - val_loss: 6.6648 - val_acc: 0.0608\n",
      "Epoch 292/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.4151 - acc: 0.3906 - val_loss: 6.6660 - val_acc: 0.0616\n",
      "Epoch 293/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.4139 - acc: 0.3924 - val_loss: 6.6788 - val_acc: 0.0608\n",
      "Epoch 294/500\n",
      "1148/1148 [==============================] - ETA: 0s - loss: 1.4061 - acc: 0.393 - 1s 681us/step - loss: 1.4111 - acc: 0.3921 - val_loss: 6.6624 - val_acc: 0.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.4058 - acc: 0.3944 - val_loss: 6.6754 - val_acc: 0.0628\n",
      "Epoch 296/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.4036 - acc: 0.3948 - val_loss: 6.6988 - val_acc: 0.0622\n",
      "Epoch 297/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.3988 - acc: 0.3954 - val_loss: 6.6896 - val_acc: 0.0602\n",
      "Epoch 298/500\n",
      "1148/1148 [==============================] - 1s 688us/step - loss: 1.3982 - acc: 0.3955 - val_loss: 6.7024 - val_acc: 0.0634\n",
      "Epoch 299/500\n",
      "1148/1148 [==============================] - 1s 701us/step - loss: 1.3996 - acc: 0.3942 - val_loss: 6.6982 - val_acc: 0.0613\n",
      "Epoch 300/500\n",
      "1148/1148 [==============================] - 1s 699us/step - loss: 1.3993 - acc: 0.3958 - val_loss: 6.6984 - val_acc: 0.0651\n",
      "Epoch 301/500\n",
      "1148/1148 [==============================] - 1s 698us/step - loss: 1.3949 - acc: 0.3938 - val_loss: 6.7318 - val_acc: 0.0608\n",
      "Epoch 302/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.3902 - acc: 0.3963 - val_loss: 6.7089 - val_acc: 0.0648\n",
      "Epoch 303/500\n",
      "1148/1148 [==============================] - 1s 698us/step - loss: 1.3862 - acc: 0.3963 - val_loss: 6.7296 - val_acc: 0.0619\n",
      "Epoch 304/500\n",
      "1148/1148 [==============================] - 1s 695us/step - loss: 1.3846 - acc: 0.3987 - val_loss: 6.7241 - val_acc: 0.0613\n",
      "Epoch 305/500\n",
      "1148/1148 [==============================] - 1s 711us/step - loss: 1.3814 - acc: 0.3991 - val_loss: 6.7376 - val_acc: 0.0605\n",
      "Epoch 306/500\n",
      "1148/1148 [==============================] - 1s 700us/step - loss: 1.3796 - acc: 0.3986 - val_loss: 6.7295 - val_acc: 0.0639\n",
      "Epoch 307/500\n",
      "1148/1148 [==============================] - 1s 701us/step - loss: 1.3783 - acc: 0.3992 - val_loss: 6.7277 - val_acc: 0.0622\n",
      "Epoch 308/500\n",
      "1148/1148 [==============================] - 1s 692us/step - loss: 1.3794 - acc: 0.3974 - val_loss: 6.7294 - val_acc: 0.0602\n",
      "Epoch 309/500\n",
      "1148/1148 [==============================] - 1s 688us/step - loss: 1.3764 - acc: 0.3996 - val_loss: 6.7421 - val_acc: 0.0619\n",
      "Epoch 310/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.3754 - acc: 0.3995 - val_loss: 6.7453 - val_acc: 0.0619\n",
      "Epoch 311/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 1.3738 - acc: 0.4019 - val_loss: 6.7371 - val_acc: 0.0605\n",
      "Epoch 312/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.3735 - acc: 0.4010 - val_loss: 6.7433 - val_acc: 0.0613\n",
      "Epoch 313/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.3738 - acc: 0.3995 - val_loss: 6.7588 - val_acc: 0.0611\n",
      "Epoch 314/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.3719 - acc: 0.3989 - val_loss: 6.7506 - val_acc: 0.0622\n",
      "Epoch 315/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 1.3700 - acc: 0.3986 - val_loss: 6.7557 - val_acc: 0.0611\n",
      "Epoch 316/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.3671 - acc: 0.4017 - val_loss: 6.7476 - val_acc: 0.0611\n",
      "Epoch 317/500\n",
      "1148/1148 [==============================] - 1s 691us/step - loss: 1.3660 - acc: 0.4012 - val_loss: 6.7652 - val_acc: 0.0637\n",
      "Epoch 318/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 1.3626 - acc: 0.4037 - val_loss: 6.7597 - val_acc: 0.0634\n",
      "Epoch 319/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.3619 - acc: 0.4036 - val_loss: 6.7604 - val_acc: 0.0631\n",
      "Epoch 320/500\n",
      "1148/1148 [==============================] - 1s 703us/step - loss: 1.3572 - acc: 0.4029 - val_loss: 6.7741 - val_acc: 0.0625\n",
      "Epoch 321/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.3554 - acc: 0.4051 - val_loss: 6.7764 - val_acc: 0.0634\n",
      "Epoch 322/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.3532 - acc: 0.4033 - val_loss: 6.7792 - val_acc: 0.0622\n",
      "Epoch 323/500\n",
      "1148/1148 [==============================] - 1s 688us/step - loss: 1.3512 - acc: 0.4038 - val_loss: 6.7912 - val_acc: 0.0634\n",
      "Epoch 324/500\n",
      "1148/1148 [==============================] - 1s 701us/step - loss: 1.3513 - acc: 0.4037 - val_loss: 6.7797 - val_acc: 0.0608\n",
      "Epoch 325/500\n",
      "1148/1148 [==============================] - 1s 689us/step - loss: 1.3464 - acc: 0.4055 - val_loss: 6.7891 - val_acc: 0.0596\n",
      "Epoch 326/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.3457 - acc: 0.4057 - val_loss: 6.7840 - val_acc: 0.0648\n",
      "Epoch 327/500\n",
      "1148/1148 [==============================] - 1s 688us/step - loss: 1.3420 - acc: 0.4064 - val_loss: 6.7917 - val_acc: 0.0639\n",
      "Epoch 328/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 1.3402 - acc: 0.4069 - val_loss: 6.8074 - val_acc: 0.0628\n",
      "Epoch 329/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.3394 - acc: 0.4098 - val_loss: 6.8064 - val_acc: 0.0613\n",
      "Epoch 330/500\n",
      "1148/1148 [==============================] - 1s 689us/step - loss: 1.3398 - acc: 0.4064 - val_loss: 6.8001 - val_acc: 0.0605\n",
      "Epoch 331/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.3397 - acc: 0.4082 - val_loss: 6.8130 - val_acc: 0.0613\n",
      "Epoch 332/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.3374 - acc: 0.4066 - val_loss: 6.8061 - val_acc: 0.0634\n",
      "Epoch 333/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.3393 - acc: 0.4066 - val_loss: 6.8284 - val_acc: 0.0608\n",
      "Epoch 334/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.3383 - acc: 0.4042 - val_loss: 6.8087 - val_acc: 0.0639\n",
      "Epoch 335/500\n",
      "1148/1148 [==============================] - 1s 696us/step - loss: 1.3380 - acc: 0.4074 - val_loss: 6.8474 - val_acc: 0.0616\n",
      "Epoch 336/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.3387 - acc: 0.4085 - val_loss: 6.8160 - val_acc: 0.0651\n",
      "Epoch 337/500\n",
      "1148/1148 [==============================] - 1s 692us/step - loss: 1.3380 - acc: 0.4062 - val_loss: 6.8493 - val_acc: 0.0619\n",
      "Epoch 338/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.3338 - acc: 0.4072 - val_loss: 6.8412 - val_acc: 0.0622\n",
      "Epoch 339/500\n",
      "1148/1148 [==============================] - 1s 689us/step - loss: 1.3286 - acc: 0.4098 - val_loss: 6.8665 - val_acc: 0.0599\n",
      "Epoch 340/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.3245 - acc: 0.4113 - val_loss: 6.8493 - val_acc: 0.0611\n",
      "Epoch 341/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 1.3225 - acc: 0.4127 - val_loss: 6.8660 - val_acc: 0.0611\n",
      "Epoch 342/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.3200 - acc: 0.4114 - val_loss: 6.8506 - val_acc: 0.0611\n",
      "Epoch 343/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.3206 - acc: 0.4114 - val_loss: 6.8779 - val_acc: 0.0590\n",
      "Epoch 344/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.3193 - acc: 0.4116 - val_loss: 6.8619 - val_acc: 0.0628\n",
      "Epoch 345/500\n",
      "1148/1148 [==============================] - 1s 687us/step - loss: 1.3159 - acc: 0.4117 - val_loss: 6.8678 - val_acc: 0.0587\n",
      "Epoch 346/500\n",
      "1148/1148 [==============================] - 1s 709us/step - loss: 1.3143 - acc: 0.4124 - val_loss: 6.8675 - val_acc: 0.0634\n",
      "Epoch 347/500\n",
      "1148/1148 [==============================] - 1s 700us/step - loss: 1.3127 - acc: 0.4121 - val_loss: 6.8697 - val_acc: 0.0584\n",
      "Epoch 348/500\n",
      "1148/1148 [==============================] - 1s 709us/step - loss: 1.3097 - acc: 0.4122 - val_loss: 6.8734 - val_acc: 0.0619\n",
      "Epoch 349/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 1.3094 - acc: 0.4137 - val_loss: 6.8757 - val_acc: 0.0596\n",
      "Epoch 350/500\n",
      "1148/1148 [==============================] - 1s 698us/step - loss: 1.3087 - acc: 0.4141 - val_loss: 6.8689 - val_acc: 0.0645\n",
      "Epoch 351/500\n",
      "1148/1148 [==============================] - 1s 694us/step - loss: 1.3091 - acc: 0.4141 - val_loss: 6.8671 - val_acc: 0.0605\n",
      "Epoch 352/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 1.3112 - acc: 0.4126 - val_loss: 6.8798 - val_acc: 0.0608\n",
      "Epoch 353/500\n",
      "1148/1148 [==============================] - 1s 695us/step - loss: 1.3091 - acc: 0.4127 - val_loss: 6.8669 - val_acc: 0.0613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 1.3073 - acc: 0.4141 - val_loss: 6.8964 - val_acc: 0.0582\n",
      "Epoch 355/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.3041 - acc: 0.4149 - val_loss: 6.8784 - val_acc: 0.0590\n",
      "Epoch 356/500\n",
      "1148/1148 [==============================] - 1s 670us/step - loss: 1.3017 - acc: 0.4133 - val_loss: 6.8934 - val_acc: 0.0611\n",
      "Epoch 357/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.2998 - acc: 0.4141 - val_loss: 6.8865 - val_acc: 0.0622\n",
      "Epoch 358/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 1.2952 - acc: 0.4152 - val_loss: 6.8956 - val_acc: 0.0616\n",
      "Epoch 359/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.2916 - acc: 0.4166 - val_loss: 6.9030 - val_acc: 0.0616\n",
      "Epoch 360/500\n",
      "1148/1148 [==============================] - 1s 676us/step - loss: 1.2908 - acc: 0.4182 - val_loss: 6.9028 - val_acc: 0.0628\n",
      "Epoch 361/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.2912 - acc: 0.4182 - val_loss: 6.9114 - val_acc: 0.0616\n",
      "Epoch 362/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2913 - acc: 0.4172 - val_loss: 6.9145 - val_acc: 0.0625\n",
      "Epoch 363/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.2920 - acc: 0.4178 - val_loss: 6.9246 - val_acc: 0.0613\n",
      "Epoch 364/500\n",
      "1148/1148 [==============================] - 2s 2ms/step - loss: 1.2920 - acc: 0.4168 - val_loss: 6.9109 - val_acc: 0.0628\n",
      "Epoch 365/500\n",
      "1148/1148 [==============================] - 1s 694us/step - loss: 1.2919 - acc: 0.4165 - val_loss: 6.9174 - val_acc: 0.0622\n",
      "Epoch 366/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 1.2904 - acc: 0.4185 - val_loss: 6.9178 - val_acc: 0.0628\n",
      "Epoch 367/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2862 - acc: 0.4194 - val_loss: 6.9054 - val_acc: 0.0608\n",
      "Epoch 368/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.2883 - acc: 0.4169 - val_loss: 6.9298 - val_acc: 0.0616\n",
      "Epoch 369/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.2818 - acc: 0.4183 - val_loss: 6.9366 - val_acc: 0.0613\n",
      "Epoch 370/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2796 - acc: 0.4194 - val_loss: 6.9301 - val_acc: 0.0622\n",
      "Epoch 371/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.2783 - acc: 0.4197 - val_loss: 6.9457 - val_acc: 0.0608\n",
      "Epoch 372/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.2750 - acc: 0.4212 - val_loss: 6.9467 - val_acc: 0.0625\n",
      "Epoch 373/500\n",
      "1148/1148 [==============================] - 1s 687us/step - loss: 1.2750 - acc: 0.4196 - val_loss: 6.9593 - val_acc: 0.0622\n",
      "Epoch 374/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2739 - acc: 0.4207 - val_loss: 6.9560 - val_acc: 0.0596\n",
      "Epoch 375/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2722 - acc: 0.4224 - val_loss: 6.9627 - val_acc: 0.0616\n",
      "Epoch 376/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.2714 - acc: 0.4210 - val_loss: 6.9720 - val_acc: 0.0616\n",
      "Epoch 377/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.2743 - acc: 0.4207 - val_loss: 6.9537 - val_acc: 0.0599\n",
      "Epoch 378/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2730 - acc: 0.4213 - val_loss: 6.9753 - val_acc: 0.0584\n",
      "Epoch 379/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.2722 - acc: 0.4224 - val_loss: 6.9794 - val_acc: 0.0608\n",
      "Epoch 380/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.2667 - acc: 0.4232 - val_loss: 6.9876 - val_acc: 0.0616\n",
      "Epoch 381/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2639 - acc: 0.4216 - val_loss: 6.9723 - val_acc: 0.0602\n",
      "Epoch 382/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2637 - acc: 0.4235 - val_loss: 6.9895 - val_acc: 0.0602\n",
      "Epoch 383/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.2616 - acc: 0.4246 - val_loss: 6.9798 - val_acc: 0.0622\n",
      "Epoch 384/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.2595 - acc: 0.4255 - val_loss: 6.9967 - val_acc: 0.0634\n",
      "Epoch 385/500\n",
      "1148/1148 [==============================] - 1s 699us/step - loss: 1.2588 - acc: 0.4238 - val_loss: 6.9694 - val_acc: 0.0631\n",
      "Epoch 386/500\n",
      "1148/1148 [==============================] - 1s 694us/step - loss: 1.2580 - acc: 0.4244 - val_loss: 6.9939 - val_acc: 0.0608\n",
      "Epoch 387/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 1.2540 - acc: 0.4262 - val_loss: 6.9836 - val_acc: 0.0637\n",
      "Epoch 388/500\n",
      "1148/1148 [==============================] - 1s 687us/step - loss: 1.2527 - acc: 0.4260 - val_loss: 6.9947 - val_acc: 0.0602\n",
      "Epoch 389/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.2530 - acc: 0.4257 - val_loss: 6.9972 - val_acc: 0.0590\n",
      "Epoch 390/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2514 - acc: 0.4247 - val_loss: 7.0057 - val_acc: 0.0587\n",
      "Epoch 391/500\n",
      "1148/1148 [==============================] - 2s 2ms/step - loss: 1.2491 - acc: 0.4279 - val_loss: 7.0057 - val_acc: 0.0582\n",
      "Epoch 392/500\n",
      "1148/1148 [==============================] - 1s 715us/step - loss: 1.2461 - acc: 0.4270 - val_loss: 7.0120 - val_acc: 0.0590\n",
      "Epoch 393/500\n",
      "1148/1148 [==============================] - 1s 711us/step - loss: 1.2443 - acc: 0.4268 - val_loss: 7.0166 - val_acc: 0.0596\n",
      "Epoch 394/500\n",
      "1148/1148 [==============================] - 1s 708us/step - loss: 1.2422 - acc: 0.4280 - val_loss: 7.0172 - val_acc: 0.0611\n",
      "Epoch 395/500\n",
      "1148/1148 [==============================] - 1s 703us/step - loss: 1.2476 - acc: 0.4249 - val_loss: 7.0192 - val_acc: 0.0622\n",
      "Epoch 396/500\n",
      "1148/1148 [==============================] - 1s 702us/step - loss: 1.2574 - acc: 0.4246 - val_loss: 7.0321 - val_acc: 0.0611\n",
      "Epoch 397/500\n",
      "1148/1148 [==============================] - 1s 711us/step - loss: 1.2530 - acc: 0.4250 - val_loss: 7.0247 - val_acc: 0.0587\n",
      "Epoch 398/500\n",
      "1148/1148 [==============================] - 1s 705us/step - loss: 1.2507 - acc: 0.4260 - val_loss: 7.0445 - val_acc: 0.0584\n",
      "Epoch 399/500\n",
      "1148/1148 [==============================] - 1s 718us/step - loss: 1.2498 - acc: 0.4246 - val_loss: 7.0441 - val_acc: 0.0582\n",
      "Epoch 400/500\n",
      "1148/1148 [==============================] - 1s 727us/step - loss: 1.2478 - acc: 0.4252 - val_loss: 7.0341 - val_acc: 0.0561\n",
      "Epoch 401/500\n",
      "1148/1148 [==============================] - 1s 718us/step - loss: 1.2472 - acc: 0.4250 - val_loss: 7.0573 - val_acc: 0.0576\n",
      "Epoch 402/500\n",
      "1148/1148 [==============================] - 1s 692us/step - loss: 1.2384 - acc: 0.4288 - val_loss: 7.0275 - val_acc: 0.0584\n",
      "Epoch 403/500\n",
      "1148/1148 [==============================] - 1s 704us/step - loss: 1.2337 - acc: 0.4296 - val_loss: 7.0339 - val_acc: 0.0579\n",
      "Epoch 404/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 1.2326 - acc: 0.4300 - val_loss: 7.0396 - val_acc: 0.0573\n",
      "Epoch 405/500\n",
      "1148/1148 [==============================] - 1s 700us/step - loss: 1.2327 - acc: 0.4289 - val_loss: 7.0577 - val_acc: 0.0582\n",
      "Epoch 406/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 1.2309 - acc: 0.4307 - val_loss: 7.0438 - val_acc: 0.0573\n",
      "Epoch 407/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 1.2259 - acc: 0.4311 - val_loss: 7.0557 - val_acc: 0.0596\n",
      "Epoch 408/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 1.2253 - acc: 0.4309 - val_loss: 7.0623 - val_acc: 0.0582\n",
      "Epoch 409/500\n",
      "1148/1148 [==============================] - 1s 671us/step - loss: 1.2267 - acc: 0.4315 - val_loss: 7.0550 - val_acc: 0.0593\n",
      "Epoch 410/500\n",
      "1148/1148 [==============================] - 1s 675us/step - loss: 1.2280 - acc: 0.4305 - val_loss: 7.0606 - val_acc: 0.0582\n",
      "Epoch 411/500\n",
      "1148/1148 [==============================] - 1s 676us/step - loss: 1.2251 - acc: 0.4318 - val_loss: 7.0717 - val_acc: 0.0564\n",
      "Epoch 412/500\n",
      "1148/1148 [==============================] - 1s 675us/step - loss: 1.2223 - acc: 0.4319 - val_loss: 7.0672 - val_acc: 0.0579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 1.2203 - acc: 0.4325 - val_loss: 7.0848 - val_acc: 0.0587\n",
      "Epoch 414/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2182 - acc: 0.4337 - val_loss: 7.0798 - val_acc: 0.0579\n",
      "Epoch 415/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 1.2215 - acc: 0.4310 - val_loss: 7.0866 - val_acc: 0.0576\n",
      "Epoch 416/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.2228 - acc: 0.4311 - val_loss: 7.0940 - val_acc: 0.0587\n",
      "Epoch 417/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.2238 - acc: 0.4318 - val_loss: 7.0895 - val_acc: 0.0564\n",
      "Epoch 418/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2231 - acc: 0.4299 - val_loss: 7.0911 - val_acc: 0.0593\n",
      "Epoch 419/500\n",
      "1148/1148 [==============================] - 2s 2ms/step - loss: 1.2231 - acc: 0.4284 - val_loss: 7.0713 - val_acc: 0.0576\n",
      "Epoch 420/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.2210 - acc: 0.4323 - val_loss: 7.1055 - val_acc: 0.0593\n",
      "Epoch 421/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.2163 - acc: 0.4327 - val_loss: 7.0935 - val_acc: 0.0564\n",
      "Epoch 422/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 1.2110 - acc: 0.4326 - val_loss: 7.1014 - val_acc: 0.0576\n",
      "Epoch 423/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 1.2099 - acc: 0.4315 - val_loss: 7.1089 - val_acc: 0.0584\n",
      "Epoch 424/500\n",
      "1148/1148 [==============================] - 1s 696us/step - loss: 1.2078 - acc: 0.4341 - val_loss: 7.1065 - val_acc: 0.0576\n",
      "Epoch 425/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.2100 - acc: 0.4319 - val_loss: 7.1150 - val_acc: 0.0570\n",
      "Epoch 426/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.2070 - acc: 0.4342 - val_loss: 7.1153 - val_acc: 0.0567\n",
      "Epoch 427/500\n",
      "1148/1148 [==============================] - 1s 689us/step - loss: 1.2082 - acc: 0.4337 - val_loss: 7.1066 - val_acc: 0.0587\n",
      "Epoch 428/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 1.2140 - acc: 0.4318 - val_loss: 7.1174 - val_acc: 0.0608\n",
      "Epoch 429/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.2117 - acc: 0.4323 - val_loss: 7.1188 - val_acc: 0.0596\n",
      "Epoch 430/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.2089 - acc: 0.4357 - val_loss: 7.1213 - val_acc: 0.0593\n",
      "Epoch 431/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.2076 - acc: 0.4342 - val_loss: 7.1270 - val_acc: 0.0567\n",
      "Epoch 432/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.2021 - acc: 0.4350 - val_loss: 7.1205 - val_acc: 0.0590\n",
      "Epoch 433/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.2010 - acc: 0.4345 - val_loss: 7.1299 - val_acc: 0.0593\n",
      "Epoch 434/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.1968 - acc: 0.4365 - val_loss: 7.1155 - val_acc: 0.0582\n",
      "Epoch 435/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.1940 - acc: 0.4388 - val_loss: 7.1357 - val_acc: 0.0579\n",
      "Epoch 436/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.1909 - acc: 0.4395 - val_loss: 7.1418 - val_acc: 0.0547\n",
      "Epoch 437/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.1893 - acc: 0.4391 - val_loss: 7.1384 - val_acc: 0.0584\n",
      "Epoch 438/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.1885 - acc: 0.4371 - val_loss: 7.1347 - val_acc: 0.0584\n",
      "Epoch 439/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.1873 - acc: 0.4400 - val_loss: 7.1449 - val_acc: 0.0582\n",
      "Epoch 440/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.1854 - acc: 0.4390 - val_loss: 7.1399 - val_acc: 0.0573\n",
      "Epoch 441/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.1853 - acc: 0.4410 - val_loss: 7.1478 - val_acc: 0.0576\n",
      "Epoch 442/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.1825 - acc: 0.4400 - val_loss: 7.1443 - val_acc: 0.0558\n",
      "Epoch 443/500\n",
      "1148/1148 [==============================] - 1s 704us/step - loss: 1.1816 - acc: 0.4408 - val_loss: 7.1525 - val_acc: 0.0602\n",
      "Epoch 444/500\n",
      "1148/1148 [==============================] - 1s 715us/step - loss: 1.1799 - acc: 0.4404 - val_loss: 7.1574 - val_acc: 0.0587\n",
      "Epoch 445/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 1.1775 - acc: 0.4419 - val_loss: 7.1682 - val_acc: 0.0596\n",
      "Epoch 446/500\n",
      "1148/1148 [==============================] - 2s 2ms/step - loss: 1.1764 - acc: 0.4416 - val_loss: 7.1502 - val_acc: 0.0570\n",
      "Epoch 447/500\n",
      "1148/1148 [==============================] - 1s 699us/step - loss: 1.1762 - acc: 0.4408 - val_loss: 7.1773 - val_acc: 0.0564\n",
      "Epoch 448/500\n",
      "1148/1148 [==============================] - 1s 719us/step - loss: 1.1732 - acc: 0.4418 - val_loss: 7.1734 - val_acc: 0.0570\n",
      "Epoch 449/500\n",
      "1148/1148 [==============================] - 1s 726us/step - loss: 1.1715 - acc: 0.4439 - val_loss: 7.1813 - val_acc: 0.0556\n",
      "Epoch 450/500\n",
      "1148/1148 [==============================] - 1s 729us/step - loss: 1.1701 - acc: 0.4453 - val_loss: 7.1847 - val_acc: 0.0558\n",
      "Epoch 451/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.1727 - acc: 0.4444 - val_loss: 7.1543 - val_acc: 0.0596\n",
      "Epoch 452/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 1.1725 - acc: 0.4430 - val_loss: 7.1788 - val_acc: 0.0567\n",
      "Epoch 453/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.1710 - acc: 0.4435 - val_loss: 7.1830 - val_acc: 0.0590\n",
      "Epoch 454/500\n",
      "1148/1148 [==============================] - 1s 688us/step - loss: 1.1706 - acc: 0.4442 - val_loss: 7.1776 - val_acc: 0.0599\n",
      "Epoch 455/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.1720 - acc: 0.4431 - val_loss: 7.1924 - val_acc: 0.0593\n",
      "Epoch 456/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.1720 - acc: 0.4443 - val_loss: 7.1937 - val_acc: 0.0596\n",
      "Epoch 457/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.1682 - acc: 0.4425 - val_loss: 7.1989 - val_acc: 0.0561\n",
      "Epoch 458/500\n",
      "1148/1148 [==============================] - 1s 673us/step - loss: 1.1664 - acc: 0.4423 - val_loss: 7.1926 - val_acc: 0.0582\n",
      "Epoch 459/500\n",
      "1148/1148 [==============================] - 1s 672us/step - loss: 1.1627 - acc: 0.4461 - val_loss: 7.1993 - val_acc: 0.0573\n",
      "Epoch 460/500\n",
      "1148/1148 [==============================] - 1s 671us/step - loss: 1.1612 - acc: 0.4437 - val_loss: 7.2065 - val_acc: 0.0547\n",
      "Epoch 461/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.1627 - acc: 0.4449 - val_loss: 7.2018 - val_acc: 0.0570\n",
      "Epoch 462/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.1599 - acc: 0.4469 - val_loss: 7.2060 - val_acc: 0.0561\n",
      "Epoch 463/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.1581 - acc: 0.4465 - val_loss: 7.2254 - val_acc: 0.0556\n",
      "Epoch 464/500\n",
      "1148/1148 [==============================] - ETA: 0s - loss: 1.1532 - acc: 0.444 - 1s 683us/step - loss: 1.1579 - acc: 0.4453 - val_loss: 7.2370 - val_acc: 0.0561\n",
      "Epoch 465/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.1565 - acc: 0.4467 - val_loss: 7.2206 - val_acc: 0.0576\n",
      "Epoch 466/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.1580 - acc: 0.4453 - val_loss: 7.2299 - val_acc: 0.0561\n",
      "Epoch 467/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.1621 - acc: 0.4445 - val_loss: 7.2299 - val_acc: 0.0567\n",
      "Epoch 468/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.1598 - acc: 0.4453 - val_loss: 7.2282 - val_acc: 0.0576\n",
      "Epoch 469/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.1600 - acc: 0.4448 - val_loss: 7.2356 - val_acc: 0.0579\n",
      "Epoch 470/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.1551 - acc: 0.4468 - val_loss: 7.2299 - val_acc: 0.0584\n",
      "Epoch 471/500\n",
      "1148/1148 [==============================] - 1s 684us/step - loss: 1.1522 - acc: 0.4477 - val_loss: 7.2390 - val_acc: 0.0579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 472/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.1514 - acc: 0.4456 - val_loss: 7.2428 - val_acc: 0.0587\n",
      "Epoch 473/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.1519 - acc: 0.4464 - val_loss: 7.2359 - val_acc: 0.0538\n",
      "Epoch 474/500\n",
      "1148/1148 [==============================] - 1s 678us/step - loss: 1.1519 - acc: 0.4481 - val_loss: 7.2340 - val_acc: 0.0556\n",
      "Epoch 475/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.1487 - acc: 0.4476 - val_loss: 7.2445 - val_acc: 0.0567\n",
      "Epoch 476/500\n",
      "1148/1148 [==============================] - 1s 676us/step - loss: 1.1488 - acc: 0.4483 - val_loss: 7.2370 - val_acc: 0.0550\n",
      "Epoch 477/500\n",
      "1148/1148 [==============================] - 1s 711us/step - loss: 1.1523 - acc: 0.4467 - val_loss: 7.2448 - val_acc: 0.0558\n",
      "Epoch 478/500\n",
      "1148/1148 [==============================] - 1s 685us/step - loss: 1.1491 - acc: 0.4474 - val_loss: 7.2627 - val_acc: 0.0576\n",
      "Epoch 479/500\n",
      "1148/1148 [==============================] - 1s 677us/step - loss: 1.1487 - acc: 0.4481 - val_loss: 7.2490 - val_acc: 0.0567\n",
      "Epoch 480/500\n",
      "1148/1148 [==============================] - 1s 670us/step - loss: 1.1456 - acc: 0.4495 - val_loss: 7.2442 - val_acc: 0.0567\n",
      "Epoch 481/500\n",
      "1148/1148 [==============================] - 1s 682us/step - loss: 1.1440 - acc: 0.4504 - val_loss: 7.2549 - val_acc: 0.0556\n",
      "Epoch 482/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 1.1448 - acc: 0.4483 - val_loss: 7.2475 - val_acc: 0.0532\n",
      "Epoch 483/500\n",
      "1148/1148 [==============================] - 1s 697us/step - loss: 1.1448 - acc: 0.4493 - val_loss: 7.2558 - val_acc: 0.0558\n",
      "Epoch 484/500\n",
      "1148/1148 [==============================] - 1s 683us/step - loss: 1.1416 - acc: 0.4512 - val_loss: 7.2690 - val_acc: 0.0532\n",
      "Epoch 485/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.1533 - acc: 0.4461 - val_loss: 7.2633 - val_acc: 0.0561\n",
      "Epoch 486/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 1.1600 - acc: 0.4454 - val_loss: 7.2618 - val_acc: 0.0541\n",
      "Epoch 487/500\n",
      "1148/1148 [==============================] - 1s 681us/step - loss: 1.1568 - acc: 0.4454 - val_loss: 7.2343 - val_acc: 0.0556\n",
      "Epoch 488/500\n",
      "1148/1148 [==============================] - 1s 698us/step - loss: 1.1539 - acc: 0.4450 - val_loss: 7.2689 - val_acc: 0.0541\n",
      "Epoch 489/500\n",
      "1148/1148 [==============================] - 1s 679us/step - loss: 1.1494 - acc: 0.4466 - val_loss: 7.2503 - val_acc: 0.0553\n",
      "Epoch 490/500\n",
      "1148/1148 [==============================] - 1s 687us/step - loss: 1.1460 - acc: 0.4476 - val_loss: 7.2500 - val_acc: 0.0527\n",
      "Epoch 491/500\n",
      "1148/1148 [==============================] - 1s 708us/step - loss: 1.1447 - acc: 0.4486 - val_loss: 7.2839 - val_acc: 0.0532\n",
      "Epoch 492/500\n",
      "1148/1148 [==============================] - 1s 686us/step - loss: 1.1481 - acc: 0.4480 - val_loss: 7.2657 - val_acc: 0.0530\n",
      "Epoch 493/500\n",
      "1148/1148 [==============================] - 1s 700us/step - loss: 1.1489 - acc: 0.4466 - val_loss: 7.2503 - val_acc: 0.0573\n",
      "Epoch 494/500\n",
      "1148/1148 [==============================] - 1s 690us/step - loss: 1.1439 - acc: 0.4479 - val_loss: 7.2695 - val_acc: 0.0532\n",
      "Epoch 495/500\n",
      "1148/1148 [==============================] - 1s 695us/step - loss: 1.1429 - acc: 0.4490 - val_loss: 7.2588 - val_acc: 0.0550\n",
      "Epoch 496/500\n",
      "1148/1148 [==============================] - 1s 693us/step - loss: 1.1392 - acc: 0.4487 - val_loss: 7.2778 - val_acc: 0.0538\n",
      "Epoch 497/500\n",
      "1148/1148 [==============================] - 1s 734us/step - loss: 1.1365 - acc: 0.4504 - val_loss: 7.2667 - val_acc: 0.0541\n",
      "Epoch 498/500\n",
      "1148/1148 [==============================] - 1s 759us/step - loss: 1.1309 - acc: 0.4530 - val_loss: 7.2843 - val_acc: 0.0541\n",
      "Epoch 499/500\n",
      "1148/1148 [==============================] - 1s 712us/step - loss: 1.1287 - acc: 0.4514 - val_loss: 7.2789 - val_acc: 0.0553\n",
      "Epoch 500/500\n",
      "1148/1148 [==============================] - 1s 680us/step - loss: 1.1278 - acc: 0.4530 - val_loss: 7.3053 - val_acc: 0.0541\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "z = np.zeros((len(input_sequences), LATENT_DIM))\n",
    "r = model.fit(\n",
    "    [input_sequences, z, z],\n",
    "    one_hot_targets,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    #callbacks=callback_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xd4nMW59/HvbNGupFUvVrMk914R\nwTbGBoeYZooTEiCEfuAEEkI4CRBOAoGcdPISUgiBEEoCSWgpBNPBsTEY2xK49yZbkq3epd3Vrub9\nY9ZgjGRL9nbdn+vaS7vPPlrdI8SPYZ55ZpTWGiGEELHDEukChBBCDI4EtxBCxBgJbiGEiDES3EII\nEWMkuIUQIsZIcAshRIyR4BZCiBgjwS2EEDFGglsIIWKMLRQfmp2drUtLS0Px0UIIEZcqKioatNY5\nAzk3JMFdWlpKeXl5KD5aCCHiklKqcqDnylCJEELEGAluIYSIMRLcQggRY0Iyxt2Xnp4eqqqqcLvd\n4fqRMcnpdFJUVITdbo90KUKIKBW24K6qqiIlJYXS0lKUUuH6sTFFa01jYyNVVVWMGDEi0uUIIaJU\n2IZK3G43WVlZEtpHoZQiKytL/q9ECHFUYR3jltA+NvkdCSGORS5OCiFEMOx5B1b+DsKwHWTYxrij\ngcvloqOjI9JlCCGinbsNElxgsYC3C/avgpxx4GkHmwNe+y5MvAgcLti7AqorYN9KyBwFJ10FCckh\nLW9IBbcQQtDZAG01kD4cerqhaQ80bAdlAb8X2qrh/d9D5ghQVug4CJ31n/6crS998nXWGPjCoyEP\nbRiiwa215vbbb+eVV15BKcX3vvc9LrnkEg4cOMAll1xCW1sbPp+Phx56iDlz5nDddddRXl6OUopr\nr72WW2+9NdJNEEIMVI8bdC9UrYb2WnjlNnC3Hv17rA7wdIAtAYZNhgnnw+o/QP0WyB4LU78EnY3Q\n3QwTL4DciSbowyQiwX3vvzexuaYtqJ85sSCV758/aUDn/v3vf2ft2rWsW7eOhoYGTj75ZObNm8df\n/vIXzjrrLL773e/i9/vp6upi7dq1VFdXs3HjRgBaWlqCWrcQIgg87VCzFuo2m0fOBGjcAZXvQcMO\n6O35+NzUQig9zYRtYjok50DOeLDaTY87KRtSC+DIiQIzr4SOOkgrDG/b+jAke9wrVqzgsssuw2q1\nMmzYMObPn8+aNWs4+eSTufbaa+np6eGiiy5i+vTpjBw5kt27d3PzzTdz3nnnsXDhwkiXL8TQ1VoF\nHbXg83z8fNdS2PVW3+enFsGsrwIKisrAmQaFZWZserCs9qgIbYhQcA+0Zxwqup+rvvPmzWP58uUs\nWbKEK664gttuu40rr7ySdevW8dprr/Hggw/y7LPP8thjj4W5YiGGgB636RnXbYXaDWZ4o7UaGndC\n+wGoKgf6+Hc3Jd+Esc8D5/8Kupvgw6fggl+DI/XTPec4cMzgVkqNA5457NBI4G6t9QMhqyrE5s2b\nx8MPP8xVV11FU1MTy5cv57777qOyspLCwkKuv/56Ojs7+eCDDzj33HNJSEjgC1/4AqNGjeLqq6+O\ndPlCxDatoX4b7HvPhHFPFxxYD027zAVC3fvxucpqernWBBPAI06Hk642QyOZIyC92AS39YglIsZ8\nLpwtCrtjBrfWehswHUApZQWqgX+EuK6QWrx4MStXrmTatGkopfj5z39OXl4eTz75JPfddx92ux2X\ny8Wf/vQnqqurueaaa+jtNX9MP/nJTyJcvRBRztsFnXWw620omWtCduPz4O00j/2roHW/OTchBaw2\nc5EPYMYVMGIeDJtkZnLkTzPDGwC9fhPscdiDHizV37BBnycrtRD4vtb61KOdV1ZWpo/cSGHLli1M\nmDDhuIocauR3JWKK1lC7yQxpbHkRNr5wxAkK0KbXbHVA7ngong3jzoWCGWZe9OZ/wfBTIDU/Ei2I\nCkqpCq112UDOHewY96XAXwdfkhAiZh0K5p4uM7TRcRAcKWZKXc1a8/C2f3x+2nCYcrE5t6vJ9KBH\nzDO95/4u7k26KDxtiRMDDm6lVAJwAXBnP+/fANwAUFxcHJTihBAR4O8xN5fUbYF1f4WWff2fmz0O\npn7RXBwcNsmMOx8a2hAhM5ge9znAB1rr2r7e1Fo/AjwCZqgkCLUJIUKp129u086daF5/+GfY8YYZ\ng/Z7Pz4vwQWzvw5dDZBRCl2NZthjypcge3RESh/qBhPclyHDJELEpqY90LwXLDZzg8reFbBn2afv\nIHSkwsgzYPplMHwWOFPNxcWUvIiULfo2oOBWSiUBnwP+O7TlCCEGbd/75maU+q2wfzXkTTHj0i2V\nYE+Elv2w//1Pfk9KAYw5y9whaLWbXnRiBiy469OzNsKw9oYYnAEFt9a6C8gKcS1CiIHw+0wQL/0x\nVH8Avu5Pvr9nGdiTTSh3N5shDmWBopNh/HlQOhcKT4pM7SIohuQt70LEjF4/WKwmoDe+YIYtNv0T\nPIcNcSTnmoWOnOlmdkZakek9H3Joyq/Mf44bEtz9ONra3Xv37mXRokUfLTwlxAk5PFhb9pmbVNY/\nYxbmry43Fwe9nXx8u3cggMeeA2MXQtm1R/98Cey4I8EtRDh4OsDmNGPJLZWmV7z5RbNIUnW5ueXb\n5zYPMLd6Z44wN6W4hkHWKJh9swnhpMzItkVEXGSC+5XvwMENwf3MvClwzk/7ffuOO+6gpKSEm266\nCYB77rkHpRTLly+nubmZnp4efvjDH3LhhRcO6se63W5uvPFGysvLsdls3H///Zxxxhls2rSJa665\nBq/XS29vLy+88AIFBQV86UtfoqqqCr/fz1133cUll1xyQs0WUazHbW5aadoNf14Mnn6WMrY6zB2E\nFhu4ck1Yjz/XrMMhRB+GTI/70ksv5Zvf/OZHwf3ss8/y6quvcuutt5KamkpDQwOzZs3iggsuGNSG\nvQ8++CAAGzZsYOvWrSxcuJDt27fz+9//nltuuYXLL78cr9eL3+/n5ZdfpqCggCVLlgDQ2nqMxdxF\nbHC3Qe1GM7UurcisaOdugZdv+3RYT7vM3KhidUDe5MA60AnHt8yoGLIiE9xH6RmHyowZM6irq6Om\npob6+noyMjLIz8/n1ltvZfny5VgsFqqrq6mtrSUvb+BzVlesWMHNN98MwPjx4ykpKWH79u3Mnj2b\nH/3oR1RVVfH5z3+eMWPGMGXKFL797W9zxx13sGjRIk477bRQNVeEUk+3GYsuf8wMbex4w2x31Zdp\nl5nedMFM89U6ZPpKIoSG1F/RxRdfzPPPP8/Bgwe59NJLefrpp6mvr6eiogK73U5paSlut3tQn9nf\nIl1f/vKXOeWUU1iyZAlnnXUWjz76KAsWLKCiooKXX36ZO++8k4ULF3L33XcHo2kiVDwdoP3QXAlV\na2DzP6FypVk3WlnMuPXwz8Cpt5gtruo2m7sOPR0w43LTAxciyIZUcF966aVcf/31NDQ0sGzZMp59\n9llyc3Ox2+0sXbqUysrKQX/mvHnzePrpp1mwYAHbt29n3759jBs3jt27dzNy5Ei+8Y1vsHv3btav\nX8/48ePJzMzkK1/5Ci6XiyeeeCL4jRTHz9NhFuHvboZ1z5hbwI8c6sgea3bx7qyHebeb4Y7DlR51\n4UwhgmJIBfekSZNob2+nsLCQ/Px8Lr/8cs4//3zKysqYPn0648ePH/Rn3nTTTXz1q19lypQp2Gw2\nnnjiCRwOB8888wxPPfUUdrudvLw87r77btasWcNtt92GxWLBbrfz0EMPhaCVYtA2/RMqnoDdSz/9\nXloxTF4MWaPN6nbDpoDFEvYShTjcoNbjHihZj/vEyO8qRDoboeZDaN5j1u7Y956Z+VG/BdJLTDDb\nE83FxhHzoGS2OS7T70QYhHI9biFiw4bnoddn5k3XbTYXEyvfM8fAzOQYfoq5uWXq980YtcUa2ZqF\nGCAJ7qPYsGEDV1xxxSeOORwOVq1aFaGKRJ/8PljzB3MTS/ZoaDtgXh8uwQWzbjK96qzRZqw6ISky\n9QpxgsIa3FrrQc2RjrQpU6awdu3asP7MUAxdxZ26rVC5wiw/+q+vm950W9XH7ysrlJ4GoxaYLbK8\nnZA9BjJKIlezEEEUtuB2Op00NjaSlZUVU+EdTlprGhsbcTqdkS4l+nS3wOvfM3cibn35kyviFc+G\ns39s1o/2e0zvWsalRRwLW3AXFRVRVVVFfX19uH5kTHI6nRQVydxf/D6zS3hPJ2x7Bba9+vGKeCVz\nYdaN5qLi6M9BwfTI1ipEmIUtuO12OyNGjAjXjxOxpm6L2ZVl+6vmAmJnI9Qetp7NyDNg3m1msSXX\nMLPY0oRFkatXiAiSi5Mi/LQ2wVtVAWufgl1LzRS9Q+zJkD/V9KZL50LZNbIBrRCHkeAW4dXdAn9Y\nYJYzPbTGdNpwmHwxjDzdrPKYNVoWXRLiKCS4RWh1NcH7vzNrfexbacK6u8m8N/dWmPMNuZAoxCBJ\ncIvg8XnNbI8Nz4HPAwfWmZ1cDu3YkjPeTNObtNjs3CKEOC4S3OLE+Lyw4zXY8m/Y8bpZoOlwI0+H\n074NJXPkzkQhgkSCWwxOby9UrYaKJ81aH63VZolTgMIyGHUGJGWZ3VwmXAApwyJbrxBxSIJbHJvW\nZpfxmg9g2c+hs84cLzkVJl5kZnykF8OUiyNbpxBDxICCWymVDjwKTMZsNX2t1nplKAsTUaC7Gd79\nNez+jwltgIwRMPfHkD3ODIPIji5ChN1A/637FfCq1vpipVQCIKvzxKOOOrOSXu0mc2v5h09B815z\nUfGsn8D488yOLjJWLUREHTO4lVKpwDzgagCttRfwhrYsETY9brPTS+0mMxvE2/Hxe/nT4bJnYNzZ\nkatPCPEpA+lxjwTqgceVUtOACuAWrXVnSCsToVNVbjYUWPc3qA5seOFIM2PWaYUmsAummyVQhRBR\nZyDBbQNmAjdrrVcppX4FfAe46/CTlFI3ADcAFBcXB7tOcaJ8Xlj3VzP8UbX64+PONLjwd7LuhxAx\nZCDBXQVUaa0P7R7wPCa4P0Fr/QjwCJity4JWoTg+vX4zPr3hOdj0D6jfao7njDfLn869FYZNgvTh\nES1TCDF4xwxurfVBpdR+pdQ4rfU24LPA5tCXJo7Lzrdg6xLY/C/oajDHRsyDCefDsMlmbrVsditE\nTBvorJKbgacDM0p2A9eEriQxaD4vbH8FXr8LWirNDjDZY+Czd5tx6+zRka5QCBFEAwpurfVaYEC7\nD4swqvkQ1j8HG56FznpIyYeFP4LPXG82w5WdhoSIS3L3RKzxdsGe5fD2/0HtRtO7Hns2nHSVGRKx\nJ0a6QiFEiElwx4KDG2Hj81BdAftWmX0VbYkw++tw2rdkWVQhhhgJ7mjl88K2l80865W/A2WBYRPN\nMMioM6BgpgS2EEOUBHc00Rr2r4atL8H216Bhmzk++Qtw7i8kqIUQgAR3dPB0mF1i1v0NmnaZC4sF\nM+Dix2HUAkhMj3SFQogoIsEdSVUV8METJrD9XiieA7NvgomLITkr0tUJIaKUBHe49bhhzR/Mll4H\nN5je9eQvQNl1MPzkSFcnhIgBEtzh4vfBsp/Ch09Dew3kTTUzQubeCo6USFcnhIghEtyh1tMNq/9g\nHq37zOYDix8yX4UQ4jhIcIdKR52Zd/3mPWaBp9LT4Nz7ZG1rIcQJk+AOto56ePlbZpEngLThcPkL\nMObMyNYlhIgbEtzB0rATVv7GLKHa44bTvg2FJ5nb0B2uSFcnhIgjEtwnquZDWPFL2Pwi2BxmX8bT\nvm3uchRCiBCIquC+9Zm1zBubzeIZRZEu5eh6/WYo5IM/we6lZtuv0/4HTvkquHIjXZ0QIs5FVXC/\nvukgWckJLJ4R6UqOonIlvPML2PkmuIbBmfdC2TVmCzAhhAiDqApuu82C198b6TL61loFb//Q7Nvo\nSIOFP4RTbgRrVP0KhRBDQFSljt1qoSfagtvTDu/9Ft77DfjcMO92c9NMQlKkKxNCDFFRFdwJVgte\nX5TsM9xeC2/9wGwJ1tVobphZ9ABkjoh0ZUKIIS66gjsahkq6m2H1o7Dyt+aux/HnwqyvyToiQoio\nEV3BbbXQ44tQcLvbzNKq7/0WvO0wZqHZvzFnbGTqEUKIfkRVcNttKvw9bp8Hyh+D5feZIZEJF8D8\n2yFvSnjrEEKIAYqu4A7nxcm2A/D+g7D2r9DVACPmw5nfN3c7CiFEFIuq4E6wWvCEeqikfhtUPAlr\nnwJvJ4w7B06+HkbOD+3PFUKIIBlQcCul9gLtgB/waa3Lgl6J1lzS+TQb1RhgdnA/+9Cdjpv+Dltf\nNhvvjj8XFtwF2WOC+7OEECLEBtPjPkNr3RCySpTinPYXsDqCuIpeRx28dS9s/Af0dEJKgbnL8fQ7\nITk7eD9HCCHCKKqGSrqsLpL87R8f6G6BZT8zO54XzIApX4Sq1TDxIsif2veHeDth9zKoeBx2vgVK\nwfQvm3nYEy8CizUcTRFCiJAZaHBr4HWllAYe1lo/Eopiuq0pJPk6zYtdS+FfX4P2g1AyB7b8GzY+\nb95b9TBc+SIUBS4k7l9jAr1mLWxdYnrXjlSYczPM+IoMhwgh4spAg/tUrXWNUioXeEMptVVrvfzw\nE5RSNwA3ABQXFx9XMW5rKsneDjPT459fhawx8F9vmJkeTbth77tmmt6zV8JTn4eTroaGHbBtifkA\nZxpM/WKgRz4NkjKPqw4hhIhmSuvB3WKulLoH6NBa/6K/c8rKynR5efmgi9l0/yKcbXsZlZ0I9kS4\n9jXz9UjNe+GJRdC6H1x5MONys6RqYgZY7YP+uUIIEWlKqYqBTvw4Zo9bKZUMWLTW7YHnC4EfnGCN\nffLYUpnEfmgAFj/cd2gDZJTCTe9Dw3bInw4WSyjKEUKIqDSQoZJhwD+UUofO/4vW+tVQFOO1p5on\nI+bD1EuOfrLDBYUzQ1GGEEJEtWMGt9Z6NzAtDLXQa3OaJwUzzGwQIYQQnxJVYwwuXwsAvSn5Ea5E\nCCGiV1QFd1tyKQC+vOmRLUQIIaJYVAX31pLLOdvzU7z5wb+jXggh4kVUBbfNZmOrLsYbqTW5hRAi\nBkRVcNttppyo23dSCCGiSFQFd4LVlCM9biGE6F90BXegx+3x+SNciRBCRK+oCu7URHO7epvbF+FK\nhBAiekVXcDtNcLd290S4EiGEiF5RFdxph3rcEtxCCNGvqAru1ERzB74EtxBC9C+6gtspY9xCCHEs\nURXcTrsVh80iPW4hhDiKqApuMDNL5OKkEEL0L+qCOy3RTptbglsIIfoTdcGd6rTR3CnBLYQQ/Ym6\n4J5alE55ZRN1be5IlyKEEFEp6oL7mlNL8fdqHn9vb6RLEUKIqBR1wV2SlczZk/N46v1KOjwyLVAI\nIY4UdcENcP1pI2l3+/jLqspIlyKEEFEnKoN7RnEG88fm8Ks3d1DV3BXpcoQQIqpEZXAD/GjxZADu\neGE9vb06wtUIIUT0iNrgLspI4n/Pm8C7Oxt5Qi5UCiHERwYc3Eopq1LqQ6XUS6Es6HBf/kwxnx2f\ny09f3cq2g+3h+rFCCBHVBtPjvgXYEqpC+qKU4mcXTyXVaePbz63DL0MmQggxsOBWShUB5wGPhrac\nT8t2Obj7/ElsqG7l+Yr94f7xQggRdQba434AuB2IyC6+50/NZ/rwdB54cwfuHtmPUggxtB0zuJVS\ni4A6rXXFMc67QSlVrpQqr6+vD1qBgc/m9rPGcaDVLRcqhRBD3kB63KcCFyil9gJ/AxYopZ468iSt\n9SNa6zKtdVlOTk6Qy4Q5o7M5c0Iuv3pzB9Ut3UH/fCGEiBXHDG6t9Z1a6yKtdSlwKfC21vorIa+s\nD/dcMAmAHy8J6zVSIYSIKlE7j7svRRlJXDd3BEs2HGB7rUwPFEIMTYMKbq31f7TWi0JVzEBcN3cE\nSQlWfvv2zkiWIYQQERNTPW6AjOQErphVwkvra6hs7Ix0OUIIEXYxF9xget02i4VHlu+OdClCCBF2\nMRncualOPj+zkOcqqmSnHCHEkBOTwQ1w4+mj6O3VPLhUxrqFEENLzAZ3SVYyXywbzl9W72N/k6zZ\nLYQYOmI2uAG+8dnRWC2K/3tpc6RLEUKIsInp4M5PS+SWz47l9c21vLm5NtLlCCFEWMR0cIOZYTIm\n18X3X9xEl1c2FxZCxL+YD+4Em4UfLZ5CdUs3v3hte6TLEUKIkIv54Ab4zIhMvjKrmMff20NFZXOk\nyxFCiJCKi+AG+M45EyhIS+T259fJmt1CiLgWN8Htctj48eensKu+k9+8vSPS5QghRMjETXADzB+b\nw8UnFfH7ZbvZWN0a6XKEECIk4iq4Ae46byKZyQnc9vx6evwR2WlNCCFCKu6COy3Jzg8vmsyWA238\n/j+7Il2OEEIEXdwFN8BZk/I4b2o+v3l7p2y4IISIO3EZ3AD3XjCJZIeV259fj79XR7ocIYQImrgN\n7myXg3sumMTa/S08/u6eSJcjhBBBE7fBDXDBtALOnJDLL17fxt4G2S1HCBEf4jq4lVL88KIp2C0W\n7nhhPb0yZCKEiANxHdwAeWlOvrdoAqv2NPH06n2RLkcIIU5Y3Ac3wJfKhnPamGx+vGQLu+s7Il2O\nEEKckCER3Eop7rt4Gg67hVv+thavT27MEULErmMGt1LKqZRarZRap5TapJS6NxyFBVtempOfLJ7C\nhupWHnhTln8VQsSugfS4PcACrfU0YDpwtlJqVmjLCo1zpuRzSdlwHlq2i1W7GyNdjhBCHJdjBrc2\nDg0M2wOPmJ2ecff5EynJTOLWZ9bS2tUT6XKEEGLQBjTGrZSyKqXWAnXAG1rrVaEtK3SSHTYeuHQG\nde0evvG3D/HJQlRCiBgzoODWWvu11tOBIuAzSqnJR56jlLpBKVWulCqvr68Pdp1BNX14Oj+4cDLL\nttfz45e3RrocIYQYlEHNKtFatwD/Ac7u471HtNZlWuuynJycIJUXOl8+pZir55Ty2Lt7+JvM7xZC\nxJCBzCrJUUqlB54nAmcCcdFN/d55E5g3Nofv/XMj78vFSiFEjBhIjzsfWKqUWg+swYxxvxTassLD\nZrXwm8tmUJyVxI1PVbCvsSvSJQkhxDENZFbJeq31DK31VK31ZK31D8JRWLikJdr541Un06vhuifX\n0NotM02EENFtSNw5eSwjspN56Csz2dvYydWPr6bD44t0SUII0S8J7oA5o7L5zWUzWV/VyrVPrKHL\nK+EthIhOEtyHOXtyHvd/aRrle5u45vE1dErPWwgRhSS4j3Dh9EJ+ecl01uxt4qrHVtPc6Y10SUII\n8QkS3H24cHqhGTapbmXx795llywFK4SIIhLc/Thvaj5/vf4U2t0+Fj/4Lu/taoh0SUIIAUhwH9VJ\nJZn882unMizVyZV/XM2T7+1F65hdX0sIESckuI9heGYSL9w0h3ljc/j+i5v4ryfLaejwRLosIcQQ\nJsE9AKlOO3+8qox7zp/IOzsbOPuBd1i2PboX0hJCxC8J7gFSSnH1qSN48eunkpls56rHVvODf2/G\n3eOPdGlCiCFGgnuQxuel8uLX53LV7BIee3cPFz34LlsPtkW6LCHEECLBfRycdiv3XjiZx64uo77d\nw6Jfr+Anr2yRuy2FEGEhwX0CFowfxhv/M5/FMwp5eNluPnf/cl7fdDDSZQkh4pwE9wnKTE7gvi9O\n47mvzibZYeWGP1fwX0+uoapZlogVQoSGBHeQnFyayZJvnMad54zn3Z2NnHn/Mh76zy68PtnTUggR\nXBLcQWS3Wvjv+aN481vzmTcmh5+9upVzfrWcVzYckBt3hBBBI8EdAoXpiTxyZRmPXlmGBm58+gPO\n/+0Klm6tkwAXQpwwCe4QOnPiMF7/5jz+3xen0drdwzVPrOHi36+UdU+EECdEhaIHWFZWpsvLy4P+\nubHM6+vluYr9/OatnRxsczNnVBbfWjiOk0oyIl2aECIKKKUqtNZlAzpXgju83D1+nl61j98t3Ulj\np5cF43P5n8+NZXJhWqRLE0JEkAR3DOj0+Hjivb08vGwXbW4fc0dnc93cEcwfm4PFoiJdnhAizCS4\nY0hrdw9PvV/Jn1bupbbNw5hcF1fOLuGiGYWkOO2RLk8IESYS3DHI6+tlyYYa/rhiDxur20hKsLJo\naj6LphYwa2QWCTa5jixEPAtqcCulhgN/AvKAXuARrfWvjvY9EtwnZn1VC0+/v4+X1tfQ6fWTm+Lg\nytklXHzScPLSnJEuTwgRAsEO7nwgX2v9gVIqBagALtJab+7veyS4g6Pb6+edHfX8+f1K3tnRgFIw\nb0wOl59SzILxudis0gsXIl4MJrhtxzpBa30AOBB43q6U2gIUAv0GtwiOxAQrCyflsXBSHnsaOvnH\nB1U8U76fG/5cQV6qk8UzC1k0NZ+J+akoJRc0hRgqBjXGrZQqBZYDk7XW/S5CLT3u0PH5e3l7ax1/\nWb2Pd3Y04O/VjMxOZtG0As6fms+YYSmRLlEIcRxCcnFSKeUClgE/0lr/vY/3bwBuACguLj6psrJy\n4BWL49LY4eHVTQd5ad0BVu1ppFfDuGEp5qLmtAJGZCdHukQhxAAFPbiVUnbgJeA1rfX9xzpfetzh\nV9fu5pUNB3lpfQ1r9jYDMKkglUVTCzh3Sh4lWRLiQkSzYF+cVMCTQJPW+psD+VAJ7sg60NrNkvUH\neGn9AdbubwGgODOJU0dnM3d0NnNGZZGRnBDhKoUQhwt2cM8F3gE2YKYDAvyv1vrl/r5Hgjt67G/q\n4u2tdazY2cD7uxpp9/hQCqYPT+f0sbl8ZkQm04enk5hgjXSpQgxpcgOO6JPP38v66laWb69n6bZ6\n1le1oDXYLIrJhWmcPTmPhROHMTLHFelShRhyJLjFgLR29VCxr4mKymZW7GhgXVUrAKVZSUwqTKOs\nJIPZo7IYm5si66cIEWIS3OK4VDV38ebmWlbsbGTLgTaqW7oBSE+yM2N4OjOLMygrzWRmSToOmwyt\nCBFMEtwiKKqau3hvVyPle5v4YF8LO+s6AHDaLUwtTGdGcTpTitKYUphGcWaS3AQkxAmQ4BYh0drV\nw+q9Tby3q4EP9rWwuaaVHr/5+0l12phcmMa04emUlWQwpTCNnBSHhLkQAyTBLcLC6+tle207G6pb\n2VDdysbqVjbXtOHr/TjMpxaurZkmAAALgElEQVSlM7M4nSlF6Ywd5pKeuRD9COpaJUL0J8FmYXJh\nGpML07gscKzb62d9VQtbDrSxrbaDtftb+O3SnQSynPw0JzOK05lalM7UwjQmF6WRKuuOCzEoEtwi\nqBITrJwyMotTRmZ9dKzD42NHbTubatpYubuR9VUtvLzh4EfvF2UkMirHxehcF3mpTkqykpiQn0pe\nmhO7rIAoxKfIUImIiOZOL+urW9lQ1cL22g521Xewu76T7h7/R+ck2q3MKE7n5NJMpg1PY0S2i6KM\nRAlzEZdkqEREvYzkBOaPzWH+2JyPjmmt6fD42FTTxr6mLjbXtLF6TxO/fnsHh/oXVouiKCORkqxk\nSrOSPvE1L83JxupWtEamLIq4JsEtooZSihSnnVkjs5h12FBLm7uHHbXt7K7vZF9TF3sbu6hs7OQf\n+5ppd/v6/KyMJDtTitLJS3UwJjeFuWOyGZ3rkt66iAsS3CLqpTrtnFSSyUklmZ84rrWmuauHvY2d\nVDZ2UtXUzbi8FJRSvLS+hj0NnWw50Maz5VUA2K2KCfmpjM51UZCWSH66k4K0RIZnJjEiOxmr3B0q\nYoQEt4hZSikykxPITE5gZnHGJ9773MRhHz0/0NrNyl2NbKttZ93+FlbuaqS2zf3RTBcAh81CSVYS\nBemJpCfaGZ6ZhEUpSrOTyE9LxOWwkeywkWCzkONyyObNIqIkuEXcy09L5PMziz5xzOfvpb7DQ02L\nmz0NnWw72Mbexi4OtHazuaaNurU1KAV9XbtPsFmYmJ/KhPwURuemMConmfSkBJx2CylOO1nJCTjt\nMr4uQkeCWwxJNquF/LRE8tMSOakk41Pva63x+nvZ39RFbZuHDo+PTo8Pj6+X3fUdrKtq5dWNB2nu\n2t/n5w9LdTA+L5XJhamkJybgctoYleNiXF4KqU6b3IQkTogEtxB9UErhsFkZnWt61f1p7PCwq76T\nDk8P3d5e2tw9NHZ42N3QyeaaNpbvqP9Urz3BZiE7OYHsFAfZLgfZroTAV4c5FngvN8VBWqJdQl58\nigS3ECcgy+Ugy+Xo9/3eXk2n10eb28fmmjYqGzup7/DQ0O6locNDbZubTTWtNHZ4P1oq4HBOu4Vh\nqc6PHi6HFYtSWC2KRLuVFKeNbJeD6cXpFGUkkZxglaAfAiS4hQghi8VMcUxx2ilMT+z3vN5eTWt3\nDw0dHhPsHV7q2tzUtrk52OahttXNuv0tdPf46e3V+LWm2+vH4+v9xOc4bJZP9OJdThs9/l4S7TZK\nspICj2QK0p0kJdhQgFJgs1jkgmsMkeAWIgpYLIqM5AQykhMYM6z/oZkjeXx+DrS4Wbu/hbp2Nw0d\npiff0OHlYJubznofFouiy+PnhQ/c/X6OUuBKsDE2L4W8VCeZgVoS7Va6e/xkJtmZWJDGsFQHmckJ\nuBwyTh9JEtxCxDCHzUppdjKl2cnHPNfd42dfUxeVjV3UtHTj8ZnlBbQ268k0dHjYVdfJloNtNHV6\naenqOcrPtZCT4sDlsJGeZKcwPQmXw8prm2px2C0UZSSS6rQzZ3Q204rSSEqwkphgI8luJclhJcFq\nkeA/ARLcQgwRTruVscNSGDvAHr2/V+Pu8ZNot9LQ4WFTTRuNnV6aOj3Ut5tHp9dPc6eX93Y10Njp\npawkg/QkO3VtHjY2tfLKxoN9frbVokiyW0lMsGIL3PhUmJHI5MI0sl0OHDYLvVpT1+ahJCuJkTku\nRmQnk5fqlG30kOAWQvTDalEkO0xE5KY6yU11Dur7tdZsr+2gqrmLLq+fLq8v8PXj591eP75eTa/W\n7Gvs4ulV+/AeNm5vs6hPXLS1WxW5KU7y0sxwTmZSApmuwNfAzVjpSXa8vl4sFoVFKZSCkdlmrn28\nkOAWQoSEUopxeSmMyxv4mP2h+fMeXy8+vyYjyU5tm4fdDWb1yOqWbmpb3Rxsc7O/qYt1+1to7vJ+\ntBPT0aQ4bKQ4bbic5i7Y3l5Nl9eP1WLqTHHacNqsWCwKd4+f/LRE0hLtJCVYqWzs4rVNB0lPsjM6\n1/T+u3v8ZCc7GJmTTElWMtmuhLAN/0hwCyGixqH584ev7JiXZnrYc0Zl9/k9WmvaPT6aOrw0dnpp\n7fbisFnRGny95j8Auxs6qGlx0+Hx0eH20eHxYbUoCtLNxdfyvc10eX24e3rx92qcdgttRyxgNrM4\nnU6Pjxcqquj0+j9VR4rDxvj8FJ7979khD/BjBrdS6jFgEVCntZ4c0mqEEGKQlFKkOu2kOu1HuUg7\nrJ/j/ev0+Gh3++j0+tAaRue6APMfivoODy6HjYOtbrNiZUMnexo68fh6w9LrHkiP+wngt8CfQluK\nEEJEj+TAwmJHUsqMswOMzHExMscF48Jb2zFn3GutlwNNYahFCCHEAMitUkIIEWOCFtxKqRuUUuVK\nqfL6+vpgfawQQogjBC24tdaPaK3LtNZlOTk5x/4GIYQQx0WGSoQQIsYcM7iVUn8FVgLjlFJVSqnr\nQl+WEEKI/hxzOqDW+rJwFCKEEGJgZKhECCFijNJ97YZ6oh+qVD1QeZzfng00BLGcWCBtHhqkzUPD\n8ba5RGs9oJkdIQnuE6GUKtdal0W6jnCSNg8N0uahIRxtlqESIYSIMRLcQggRY6IxuB+JdAERIG0e\nGqTNQ0PI2xx1Y9xCCCGOLhp73EIIIY4iaoJbKXW2UmqbUmqnUuo7ka4nWJRSjyml6pRSGw87lqmU\nekMptSPwNSNwXCmlfh34HaxXSs2MXOXHTyk1XCm1VCm1RSm1SSl1S+B43LZbKeVUSq1WSq0LtPne\nwPERSqlVgTY/o5RKCBx3BF7vDLxfGsn6T4RSyqqU+lAp9VLgdVy3WSm1Vym1QSm1VilVHjgW1r/t\nqAhupZQVeBA4B5gIXKaUmhjZqoLmCeDsI459B3hLaz0GeCvwGkz7xwQeNwAPhanGYPMB39JaTwBm\nAV8L/POM53Z7gAVa62nAdOBspdQs4GfALwNtbgYOLRlxHdCstR4N/DJwXqy6Bdhy2Ouh0OYztNbT\nD5v2F96/ba11xB/AbOC1w17fCdwZ6bqC2L5SYONhr7cB+YHn+cC2wPOHgcv6Oi+WH8C/gM8NlXYD\nScAHwCmYGzFsgeMf/Z0DrwGzA89tgfNUpGs/jrYWYYJqAfASoIZAm/cC2UccC+vfdlT0uIFCYP9h\nr6sCx+LVMK31AYDA19zA8bj7PQT+d3gGsIo4b3dgyGAtUAe8AewCWrTWh3adPbxdH7U58H4rkBXe\nioPiAeB2oDfwOov4b7MGXldKVSilbggcC+vfdrTs8t7X7ppDcbpLXP0elFIu4AXgm1rrtqNsohoX\n7dZa+4HpSql04B/AhL5OC3yN+TYrpQ5tIl6hlDr90OE+To2bNgecqrWuUUrlAm8opbYe5dyQtDla\netxVwPDDXhcBNRGqJRxqlVL5AIGvdYHjcfN7UErZMaH9tNb674HDcd9uAK11C/AfzPh+ulLqUAfp\n8HZ91ObA+2nE3t6upwIXKKX2An/DDJc8QHy3Ga11TeBrHeY/0J8hzH/b0RLca4AxgavRCcClwIsR\nrimUXgSuCjy/CjMGfOj4lYEr0bOA1kP/+xVLlOla/xHYorW+/7C34rbdSqmcQE8bpVQicCbmgt1S\n4OLAaUe2+dDv4mLgbR0YBI0VWus7tdZFWutSzL+zb2utLyeO26yUSlZKpRx6DiwENhLuv+1ID/Qf\nNmh/LrAdMy743UjXE8R2/RU4APRg/ut7HWZc7y1gR+BrZuBchZldswvYAJRFuv7jbPNczP8OrgfW\nBh7nxnO7ganAh4E2bwTuDhwfCawGdgLPAY7AcWfg9c7A+yMj3YYTbP/pwEvx3uZA29YFHpsOZVW4\n/7blzkkhhIgx0TJUIoQQYoAkuIUQIsZIcAshRIyR4BZCiBgjwS2EEDFGglsIIWKMBLcQQsQYCW4h\nhIgx/x/DwjO1yit8IQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21c757ce48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21c7965208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sampling model\n",
    "input2 = Input(shape=(1,))\n",
    "x = embedding_layer(input2)\n",
    "x, h, c = lstm(x, initial_state=[initial_h, initial_c])\n",
    "output2 = dense(x)\n",
    "\n",
    "# hidden, cell 을 출력할 것이다. 다시 input으로 들어가야 하므로\n",
    "sampling_model = Model([input2, initial_h, initial_c], [output2, h, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             300000      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   multiple             12600       embedding_1[1][0]                \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 multiple             78000       lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 390,600\n",
      "Trainable params: 390,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sampling_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input으로 들어가는 것이 sequence_length = 1이 되었다. 그래도 output shape은 똑같다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_line():\n",
    "    np_input = np.array([[word2idx['<sos>']]])\n",
    "    h = np.zeros((1, LATENT_DIM))\n",
    "    c = np.zeros((1, LATENT_DIM))\n",
    "                    \n",
    "    eos = word2idx['<eos>']\n",
    "    \n",
    "    output_sentence = []\n",
    "    \n",
    "    for _ in range(max_sequence_length):\n",
    "        o, h, c = sampling_model.predict([np_input, h, c])\n",
    "        probs = o[0, 0]\n",
    "        \n",
    "        # 아무것도 아닐 때 - padding\n",
    "        if np.argmax(probs) == 0:\n",
    "            print('wtf')\n",
    "            \n",
    "        # recalculate\n",
    "        probs[0] = 0\n",
    "        probs /= probs.sum()\n",
    "        idx = np.random.choice(len(probs), p=probs)\n",
    "        \n",
    "        if idx == eos:\n",
    "            break\n",
    "            \n",
    "        output_sentence.append(idx2word.get(idx, '<WTF %s>'% idx))\n",
    "        np_input[0, 0] = idx\n",
    "    return ' '.join(output_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v:k for k, v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i went whose piano's fur.\n",
      "the poetess had is nailed.\n",
      "'she should, shouldn't she, you're so many times\n",
      "so far do it that are what i don't we don't care\n"
     ]
    }
   ],
   "source": [
    "for _ in range(4):\n",
    "    print(sample_line())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
