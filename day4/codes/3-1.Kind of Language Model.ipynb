{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Autoregressive Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자기회귀 언어 모델링(Autoregressive language modeling): Sequence 가 주어졌을 때 이 문장에게 점수를 부여 하는 방법입니다.\n",
    "이전 토큰이 나왔을때 다음 토큰이 나올 확률을 계산하는 작업과 동일합니다. 이렇게 정의 하면서 비지도학습 문제를 지도학습으로 푸는 문제로 변하게 됩니다.\n",
    "즉, 이전에 공부했던 텍스트 분류 문제와 같아지는데, input 은 이전에 나온 토큰, output은 다음에 나올 토큰을 예측 하는 것입니다.\n",
    "\n",
    "\n",
    "또한, 문장에 점수(Score)를 부여하는 방식은 인간이 말하는 사고 방식과 동일합니다.\n",
    "\n",
    "- 점수는 음의 최대 우도(negative log likelihood)로 측정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 문장의 확률은 각 단어 등장 확률의 곱이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 문장의 확률은 문장 내 각 단어 등장 확률의 곱이다.\n",
    "\n",
    "\n",
    "Sentence는 일종의 단어들의 Sequence라고 볼 수 있습니다.\n",
    "즉, 이 장에서 Sequence라고 표현하는 것은 곧 Sentence를 의미함을 이해해주시기 바랍니다.\n",
    "\n",
    "어떤 Sequence가 주어졌을 때, 어떻게 Probability Distribution을 계산할 수 있을까요?\n",
    "즉, 다시 말해 Sequence에 어떻게 Probability(확률)를 부여(Scoring)할 수 있을까요?\n",
    "\n",
    "그 방법은 Sequence의 확률은 이전 단어가 주어졌을 때, 다음 단어가 나올 확률들을 전부 곱한 것으로 생각하는 것입니다.\n",
    "\n",
    "가령, P(첫번째 단어)는 첫번째 단어의 확률입니다. 두번째 단어가 나올 때는 이미 앞에서 첫번째 단어가 등장했을 것입니다. P(두번째단어 | 첫번째단어)라는 것은 첫번째 단어가 등장했을 때, 두번째 단어가 나올 확률을 의미합니다. 그렇다면 P(세번째단어 | 첫번째 단어, 두번째 단어)의 확률은 무엇을 의미할까요? 네. 첫번째 단어와 두번째 단어가 등장했을 때, 세번째 단어가 등장할 확률입니다.\n",
    "(고교 과정의 확률에 대한 이해가 있다면, P(A|B)와 같은 확률은 조건부 확률을 의미함을 아실 것입니다.)\n",
    "\n",
    "\n",
    "이를 식으로 표현하면 위와 같습니다.\n",
    "Sequence의 확률이라는 것은 Sequence 내에 있는 단어들에 대해서,\n",
    "이전 단어가 주어졌을 때 다음 단어가 나올 확률들의 곱이라는 의미를 식으로 표현한 것입니다.\n",
    "(Log Probability라면 곱이 아니라 덧셈이 될 것입니다.)\n",
    "\n",
    "즉, 실제 예제에 해당 식을 적용해보면 다음과 같습니다.\n",
    "\n",
    "P(“An adorable little boy is spreading smiles”) =\n",
    "P(An) × P(adorable|An) × P(little|An adorable)\n",
    "× P(is|An adorable little) × P(spreading|An adorable little is)\n",
    "× P(smiles|An adorable little is spreading)\n",
    "\n",
    "앞서 Sequence(Sentence)에 확률을 부여하는 것은 비지도 학습에 속한다고 했습니다.\n",
    "그런데, 이렇게 정의를 일반화하면 비지도 학습이 우리가 앞에서 계속해서 설명해왔던\n",
    "지도 학습으로 푸는 문제로 변하게 됩니다.\n",
    "\n",
    "왜 이게 지도 학습이라고 볼 수 있는 걸까요?\n",
    "P(is|An adorable little)를 봤을 때, An adorable little이 input이 주어졌을 때,\n",
    "is라는 output이 나온다고 볼 수 있는 것입니다.\n",
    "\n",
    "이것을 지도 학습 관점에서 본다면,\n",
    "우리가 앞서 지도 학습을 Neural Net에 적용했던 과정을 고스란히 이 문제에도 적용할 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 지도 학습으로 생각한다면, Text Classification으로 해결할 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/autoregressive nn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification은 어떻게 할 수 있을까요? input으로는 1부터 t-1까지의 단어들이 들어갑니다. 이 강의에서는 Sentence Representation Extract에 대해서 총 5가지를 배웠습니다.\n",
    "\n",
    "그 중 1가지 방법을 사용하여, Representation을 얻고, 그 Representation은 Arbitrary한 Sub-graph로 갑니다. 그 후에는 Softmax Classification을 합니다. Softmax를 하고 나면, Vocabulary에 있는 모든 단어에 대해서 확률이 부여된 상태일 것입니다. Training할 때는, 그 많은 확률이 부여된 단어들 중에서 실제 데이터에 있었던 단어의 확률이 높아지도록 하는 것이 될 것입니다.\n",
    "\n",
    "Loss function은 negative log probability의 합으로 하면 될 것입니다. 그리고 이것을 Data에 있는 모든 단어에 대해서 수행합니다. 다시 전체적으로 정리해서 언급해보면, training data는 그냥 Sentence가 주어졌었지만, 그것을 input과 output pair의 지도 학습의 training set으로 바꾼 뒤에, Text Classifcation 모델에 Tranining을 시키는 것입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/nnloss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ngram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/ngram1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/ngram2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nerual Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural N-Gram Language Model: 신경망을 사용함으로서 데이터 희소성(data sparsity) 문제를 해결 할 수 있습니다.\n",
    "\n",
    "기존의 카운트 기반 모델 보다 훈련 데이터에서 나오지 않았었던 N-gram 을 계산 할 수가 있었습니다. 어떻게 이것이 가능할까요? 그전에 데이터 희소성(data sparsity) 문제가 생기는 이유를 살펴봐야합니다.\n",
    "\n",
    "\n",
    "간단한 대답은 토큰들이 훈련시에는 생기지 않지만, 테스트시에 만 생기기 때문입니다. 조금더 깊은 대답은 이산 공간(discrete space) 에서 카운트하여 토큰들의 유사도 측정이 불가능 하기 때문입니다. 하지만 신경망에서는 토큰을 연속 벡터 공간(continuous vector space) 에 매핑(mapping) 시킵니다. 나오지 않았던 단어들도 유사도를 계산을 통해 연속 벡터 공간에서 의미가 있는 분포를 찾을 수 있게 됩니다. 이러한 학습을 통해 데이터 희소성 문제를 해결 할 수 있습니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
